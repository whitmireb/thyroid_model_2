{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install imbalanced-learn\n",
    "\n",
    "# commented out pips\n",
    "# replaced the data_path and base_image_path, tan_directory, df1, df2, final_df.to_csv\n",
    "# Please make sure to only add this file, and not the other green ones. I don't want to brake stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, UnidentifiedImageError \n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "import efficientnet_pytorch as Efficinet\n",
    "\n",
    "# Set paths\n",
    "data_path = \"data_NOH.csv\"  # '/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv'\n",
    "base_image_path = \"\" # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(clip.__file__)\n",
    "\n",
    "# Load the CLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to process a single image and text entry through CLIP\n",
    "def process_image_and_text(image_path, text, model, preprocess):\n",
    "    # Open image with error handling for corrupt files\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Error opening image {image_path}: {e}. Skipping.\")\n",
    "        return None, None  # Return None to indicate failure\n",
    "\n",
    "    # Process text\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    return image_features.cpu().numpy(), text_features.cpu().numpy()\n",
    "\n",
    "# Initialize the list to keep track of successfully processed image paths\n",
    "processed_image_paths = []\n",
    "\n",
    "# Iterate over the dataframe and compute embeddings with progress bar\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "# Wrap the iteration in tqdm for progress tracking\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\", unit=\"row\"):\n",
    "    img_path = os.path.join(base_image_path, row['image_path'].replace('\\\\', '/'))\n",
    "    diagnosis_text = row['Surgery diagnosis']\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # Process image and text\n",
    "        img_embed, txt_embed = process_image_and_text(img_path, diagnosis_text, model, preprocess)\n",
    "        \n",
    "        if img_embed is not None and txt_embed is not None:\n",
    "            image_embeddings.append(img_embed)\n",
    "            text_embeddings.append(txt_embed)\n",
    "            # Append the successful image path to the list\n",
    "            processed_image_paths.append(row['image_path'])\n",
    "    else:\n",
    "        print(f\"Image {img_path} does not exist. Skipping.\")\n",
    "\n",
    "# Convert embeddings to arrays and save for later use\n",
    "if image_embeddings:\n",
    "    image_embeddings = torch.cat([torch.tensor(x) for x in image_embeddings], dim=0)\n",
    "    torch.save(image_embeddings, 'image_embeddings.pt')\n",
    "\n",
    "if text_embeddings:\n",
    "    text_embeddings = torch.cat([torch.tensor(x) for x in text_embeddings], dim=0)\n",
    "    torch.save(text_embeddings, 'text_embeddings.pt')\n",
    "\n",
    "print(\"Embeddings saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "# Check the shape of the image embeddings\n",
    "print(image_embeddings.shape)\n",
    "# Check the number of image embeddings generated\n",
    "print(len(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the embeddings\n",
    "image_embeddings = torch.load('image_embeddings.pt')\n",
    "text_embeddings = torch.load('text_embeddings.pt')\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_embeddings, labels, test_size=0.25, random_state=42, stratify=labels)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model with class weight and regularization\n",
    "clf = LogisticRegression(class_weight='balanced', C=0.8)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "print(\"ROC-AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the TAN directory\n",
    "tan_directory = \"TAN\" # '/home/iambrink/NOH_Thyroid_Cancer_Data/TAN'  # Update this path to your TAN folder\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(tan_directory):\n",
    "    if filename.startswith(\"TAN\"):  # Check if the filename starts with \"TAN\"\n",
    "        # Remove the \"TAN\" prefix\n",
    "        new_filename = filename.replace(\"TAN\", \"\", 1)  # Remove \"TAN\" only once\n",
    "        new_filename = new_filename.lstrip(\"\\\\\")  # Remove any leading backslashes if necessary\n",
    "        \n",
    "        # Create full paths for renaming\n",
    "        old_file = os.path.join(tan_directory, filename)\n",
    "        new_file = os.path.join(tan_directory, new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(old_file, new_file)\n",
    "\n",
    "print(\"Files renamed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 =  pd.read_csv('data_NOH.csv') # pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('data_TAN.csv') # pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_TAN_V2.csv')\n",
    "\n",
    "# Select relevant columns from df1\n",
    "columns_df1 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',\n",
    "    'image_path',  # Keep the image_path from df1\n",
    "    'Surgery diagnosis'\n",
    "]\n",
    "\n",
    "# Select relevant columns from df2\n",
    "columns_df2 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',  # Include relevant columns from df2\n",
    "    'image_path',                    # Keep the image_path from df2\n",
    "    'Surgery diagnosis'\n",
    "]\n",
    "\n",
    "# Create DataFrames with only the selected columns\n",
    "df1_selected = df1[columns_df1]\n",
    "df2_selected = df2[columns_df2]\n",
    "\n",
    "# Get the maximum patient number from df1\n",
    "max_patient_number = df1_selected['Patient #'].max()\n",
    "\n",
    "# Update patient numbers in df2 by adding max_patient_number\n",
    "df2_selected['Patient #'] = df2_selected['Patient #'] + max_patient_number\n",
    "\n",
    "# Concatenate the two DataFrames vertically\n",
    "final_df = pd.concat([df1_selected, df2_selected], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "# final_df.to_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "final_df.to_csv('Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "\n",
    "print(\"Super CSV file with adjusted Patient # has been created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 = pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_TAN_V2.csv')\n",
    "\n",
    "# Select relevant columns from df1\n",
    "columns_df1 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',\n",
    "    'image_path'  # Keep the image_path from df1\n",
    "]\n",
    "\n",
    "# Select relevant columns from df2\n",
    "columns_df2 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',  # Include relevant columns from df2\n",
    "    'image_path'                    # Keep the image_path from df2\n",
    "]\n",
    "\n",
    "# Create DataFrames with only the selected columns\n",
    "df1_selected = df1[columns_df1]\n",
    "df2_selected = df2[columns_df2]\n",
    "\n",
    "# Get the maximum patient number from df1\n",
    "max_patient_number = df1_selected['Patient #'].max()\n",
    "\n",
    "# Update patient numbers in df2 by adding max_patient_number\n",
    "df2_selected['Patient #'] = df2_selected['Patient #'] + max_patient_number\n",
    "\n",
    "# Adjust image_path in df2_selected to format it correctly\n",
    "df2_selected['image_path'] = df2_selected['image_path'].str.replace(r'TAN/', '', regex=True)  # Remove 'TAN/' prefix\n",
    "\n",
    "# Extract the numeric part; fill NaN values with empty strings to avoid TypeError\n",
    "df2_selected['image_number'] = df2_selected['image_path'].str.extract(r'(\\d+)')  # Extract the numeric part\n",
    "\n",
    "# Construct the new image_path without duplicating 'TAN'\n",
    "df2_selected['image_path'] = 'TAN\\\\' + df2_selected['image_number'].fillna('Unknown') + '\\\\' + df2_selected['image_path'].str.replace(r'TAN\\d*\\\\', '', regex=True)\n",
    "\n",
    "# Drop the image_number column\n",
    "df2_selected = df2_selected.drop(columns=['image_number'])\n",
    "\n",
    "# Concatenate the two DataFrames vertically\n",
    "final_df = pd.concat([df1_selected, df2_selected], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "final_df.to_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "\n",
    "print(\"Super CSV file with adjusted Patient # and image_path has been created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, UnidentifiedImageError \n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "\n",
    "# Set paths\n",
    "data_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'  # Update to your super CSV file\n",
    "base_image_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/'  # Adjust based on where your images are located\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Load the CLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to process a single image and text entry through CLIP\n",
    "def process_image_and_text(image_path, text, model, preprocess):\n",
    "    # Open image with error handling for corrupt files\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Error opening image {image_path}: {e}. Skipping.\")\n",
    "        return None, None  # Return None to indicate failure\n",
    "\n",
    "    # Process text\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    return image_features.cpu().numpy(), text_features.cpu().numpy()\n",
    "\n",
    "# Initialize the list to keep track of successfully processed image paths\n",
    "processed_image_paths = []\n",
    "\n",
    "# Iterate over the dataframe and compute embeddings with progress bar\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "# Wrap the iteration in tqdm for progress tracking\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\", unit=\"row\"):\n",
    "    # Construct the full image path\n",
    "    img_path = os.path.join(base_image_path, row['image_path'].replace('\\\\', '/'))\n",
    "    diagnosis_text = str(row['Surgery diagnosis in number'])  # Use surgery diagnosis in number as text\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # Process image and text\n",
    "        img_embed, txt_embed = process_image_and_text(img_path, diagnosis_text, model, preprocess)\n",
    "        \n",
    "        if img_embed is not None and txt_embed is not None:\n",
    "            image_embeddings.append(img_embed)\n",
    "            text_embeddings.append(txt_embed)\n",
    "            # Append the successful image path to the list\n",
    "            processed_image_paths.append(row['image_path'])\n",
    "    else:\n",
    "        print(f\"Image {img_path} does not exist. Skipping.\")\n",
    "\n",
    "# Convert embeddings to arrays and save for later use\n",
    "if image_embeddings:\n",
    "    image_embeddings = torch.cat([torch.tensor(x) for x in image_embeddings], dim=0)\n",
    "    torch.save(image_embeddings, 'image_embeddings.pt')\n",
    "\n",
    "if text_embeddings:\n",
    "    text_embeddings = torch.cat([torch.tensor(x) for x in text_embeddings], dim=0)\n",
    "    torch.save(text_embeddings, 'text_embeddings.pt')\n",
    "\n",
    "print(\"Embeddings saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "# Check the shape of the image embeddings\n",
    "print(image_embeddings.shape)\n",
    "# Check the number of image embeddings generated\n",
    "print(len(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the embeddings\n",
    "image_embeddings = torch.load('image_embeddings.pt')\n",
    "text_embeddings = torch.load('text_embeddings.pt')\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have your image_embeddings and labels defined\n",
    "# Filter out rows where labels are NaN\n",
    "valid_indices = ~np.isnan(labels)\n",
    "image_embeddings_filtered = image_embeddings[valid_indices]\n",
    "labels_filtered = labels[valid_indices]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_embeddings_filtered, labels_filtered, test_size=0.25, random_state=42, stratify=labels_filtered)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression(class_weight='balanced', C=0.8)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Save the model\n",
    "dump(clf, 'logistic_regression_model.joblib')  # Using joblib\n",
    "# or\n",
    "# with open('logistic_regression_model.pkl', 'wb') as file:  # Using pickle\n",
    "#     pickle.dump(clf, file)\n",
    "\n",
    "# Load the model\n",
    "clf_loaded = load('logistic_regression_model.joblib')  # Using joblib\n",
    "# or\n",
    "# with open('logistic_regression_model.pkl', 'rb') as file:  # Using pickle\n",
    "#     clf_loaded = pickle.load(file)\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "y_pred = clf_loaded.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_test, clf_loaded.predict_proba(X_test)[:, 1])\n",
    "print(\"ROC-AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # Assuming RGB images\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, 1)  # Binary classification (output one value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 56 * 56)  # Flatten the tensor\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match CNN input size\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pdb\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from plip import PLIP\n",
    "import numpy as np\n",
    "\n",
    "plip = PLIP('vinid/plip')\n",
    "\n",
    "plip_model = CLIPModel.from_pretrained(\"vinid/plip\")\n",
    "plip_processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n",
    "plip_texts = np.unique(df['Surgery diagnosis']).tolist()\n",
    "\n",
    "# Set paths\n",
    "data_path = 'Thyroid_Cancer_TAN&NOH_file.csv' #'/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'\n",
    "base_image_path = '' # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match CNN input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create your own Dataset class to load images and labels\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, base_path, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.base_path, self.dataframe.iloc[idx]['image_path'].replace('\\\\', '/'))\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx]['Surgery diagnosis in number']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load data and prepare for training\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['Surgery diagnosis in number'])  # Drop rows with NaN labels\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = CustomDataset(train_df, base_image_path, transform=transform)\n",
    "test_dataset = CustomDataset(test_df, base_image_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "\n",
    "# Training loop with tqdm\n",
    "num_epochs = 200\n",
    "best_val_acc = 0.0  # To keep track of the best validation accuracy\n",
    "\n",
    "# Heat map using captum\n",
    "\n",
    "heat_map_data_iter = iter(test_loader)\n",
    "heat_map_images, heat_map_labels = next(heat_map_data_iter)\n",
    "test_img = heat_map_images[0]\n",
    "test_label = heat_map_labels[0]\n",
    "\n",
    "output = model(test_img)\n",
    "pred_label_idx = output.argmax(dim=1).item()\n",
    "\n",
    "occlusion = Occlusion(model)\n",
    "\n",
    "# Compute the attributions\n",
    "attributions_occ = occlusion.attribute(\n",
    "    test_img,\n",
    "    target=pred_label_idx,\n",
    "    strides=(16, 16),  # Spatial dimensions only\n",
    "    sliding_window_shapes=(32, 32),  # Spatial dimensions only\n",
    "    baselines=0\n",
    ")\n",
    "\n",
    "# Overlay the attribution on top of the original image with the chosen colormap\n",
    "_ = viz.visualize_image_attr(\n",
    "    np.expand_dims(attributions_occ.squeeze().cpu().detach().numpy(), axis=-1),\n",
    "    np.transpose(test_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "    method=\"blended_heat_map\",\n",
    "    sign=\"all\",\n",
    "    show_colorbar=True,\n",
    "    title=\"Original Image with Colored Attribution Overlay\",\n",
    "    fig_size=(8, 8),\n",
    "    use_pyplot=True,\n",
    "    alpha_overlay=0.5  # Control the transparency of the overlay (0 is fully transparent, 1 is fully opaque)\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.float().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update training metrics\n",
    "            train_loss += loss.item()\n",
    "            predictions = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)  # Increment the progress bar\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = correct_predictions / len(train_dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels.float().to(device))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predictions = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "            val_correct_predictions += (predictions == labels.float().to(device)).sum().item()\n",
    "\n",
    "            val_labels.extend(labels.numpy())\n",
    "            val_preds.extend(predictions.cpu().numpy())\n",
    "            \n",
    "            # add in plip guesses\n",
    "            for image in images:\n",
    "                plip_inputs = plip_processor(text=plip_texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "                plip_outputs = plip_model(**plip_inputs)\n",
    "                plip_logits_per_image = plip_outputs.logits_per_image  # this is the image-text similarity score\n",
    "                plip_probs = plip_logits_per_image.softmax(dim=1) \n",
    "                plip_response = plip_texts[np.argmax(plip_probs.detach().numpy() )]\n",
    "                print(plip_response)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(test_loader)\n",
    "    val_acc = val_correct_predictions / len(test_dataset)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Heat map using captum\n",
    "\n",
    "    heat_map_data_iter = iter(test_loader)\n",
    "    heat_map_images, heat_map_labels = next(heat_map_data_iter)\n",
    "    test_img = heat_map_images[0]\n",
    "    test_label = heat_map_labels[0]\n",
    "\n",
    "    output = model(test_img)\n",
    "    pred_label_idx = output.argmax(dim=1).item()\n",
    "\n",
    "    occlusion = Occlusion(model)\n",
    "\n",
    "    # Compute the attributions\n",
    "    attributions_occ = occlusion.attribute(\n",
    "        test_img,\n",
    "        target=pred_label_idx,\n",
    "        strides=(16, 16),  # Spatial dimensions only\n",
    "        sliding_window_shapes=(32, 32),  # Spatial dimensions only\n",
    "        baselines=0\n",
    "    )\n",
    "\n",
    "    # Overlay the attribution on top of the original image with the chosen colormap\n",
    "    _ = viz.visualize_image_attr(\n",
    "        np.expand_dims(attributions_occ.squeeze().cpu().detach().numpy(), axis=-1),\n",
    "        np.transpose(test_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "        method=\"blended_heat_map\",\n",
    "        sign=\"all\",\n",
    "        show_colorbar=True,\n",
    "        title=\"Original Image with Colored Attribution Overlay\",\n",
    "        fig_size=(8, 8),\n",
    "        use_pyplot=True,\n",
    "        alpha_overlay=0.5  # Control the transparency of the overlay (0 is fully transparent, 1 is fully opaque)\n",
    "    )\n",
    "\n",
    "    # Save the model if the validation accuracy improves\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'super_CNN_Model.pth')\n",
    "        print(\"Model saved with best validation accuracy!\")\n",
    "\n",
    "# Final evaluation\n",
    "model.load_state_dict(torch.load('super_CNN_Model.pth'))  # Load the best model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        predictions = torch.round(torch.sigmoid(outputs.squeeze()))  # Convert probabilities to binary predictions\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_true, y_pred)\n",
    "print(\"ROC-AUC Score:\", auc_score)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "#100 ep - 325 mins, best val acc 88.18% #200 ep - 682 min, best val acc 88.41%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model and evaluate\n",
    "model = SimpleCNN().to(device)\n",
    "model.load_state_dict(torch.load('super_CNN_Model.pth'))  # Load the best model\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Example inference on a single image\n",
    "def predict_image(image_path, model, transform, device):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title('Input Image')\n",
    "    plt.show()\n",
    "\n",
    "    # Transform and add batch dimension\n",
    "    image_transformed = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image_transformed)\n",
    "        probability = torch.sigmoid(output).item()  # Get the probability (confidence between 0-1)\n",
    "        \n",
    "        # Classify based on the probability threshold (0.5 by default)\n",
    "        if probability >= 0.5:\n",
    "            prediction = 'Cancer'\n",
    "            confidence = probability\n",
    "        else:\n",
    "            prediction = 'Non-Cancer'\n",
    "            confidence = 1 - probability  # Confidence for non-cancer is 1 - probability\n",
    "            \n",
    "    return prediction, confidence\n",
    "\n",
    "# Use the prediction function\n",
    "sample_image_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/TAN/001/IMG_20220623_134910.jpg'  # Replace with actual path\n",
    "predicted_class, confidence = predict_image(sample_image_path, model, transform, device)\n",
    "print(f'Predicted Class: {predicted_class}, Confidence: {confidence:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the plip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<00:57,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  [0.80887467 0.19112532]\n",
      "label:  tensor(0)\n",
      "prob:  [0.63799125 0.36200878]\n",
      "label:  tensor(0)\n",
      "prob:  [0.68248034 0.31751966]\n",
      "label:  tensor(0)\n",
      "prob:  [0.76160395 0.23839606]\n",
      "label:  tensor(0)\n",
      "prob:  [0.6763139  0.32368606]\n",
      "label:  tensor(0)\n",
      "prob:  [0.5399428 0.4600572]\n",
      "label:  tensor(0)\n",
      "prob:  [0.79734147 0.20265855]\n",
      "label:  tensor(0)\n",
      "prob:  [0.6296683 0.3703317]\n",
      "label:  tensor(0)\n",
      "prob:  [0.63720715 0.3627928 ]\n",
      "label:  tensor(0)\n",
      "prob:  [0.7013948 0.2986052]\n",
      "label:  tensor(0)\n",
      "prob:  [0.88375837 0.1162416 ]\n",
      "label:  tensor(0)\n",
      "prob:  [0.8002808  0.19971925]\n",
      "label:  tensor(0)\n",
      "prob:  [0.7743778  0.22562216]\n",
      "label:  tensor(0)\n",
      "prob:  [0.74110496 0.258895  ]\n",
      "label:  tensor(0)\n",
      "prob:  [0.59367 0.40633]\n",
      "label:  tensor(0)\n",
      "15/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/26 [00:04<00:55,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  [0.6382947 0.3617053]\n",
      "label:  tensor(0)\n",
      "prob:  [0.8385868  0.16141316]\n",
      "label:  tensor(0)\n",
      "prob:  [0.47828326 0.5217168 ]\n",
      "label:  tensor(1)\n",
      "prob:  [0.7292495  0.27075055]\n",
      "label:  tensor(0)\n",
      "prob:  [0.48760378 0.51239616]\n",
      "label:  tensor(1)\n",
      "prob:  [0.77222914 0.22777079]\n",
      "label:  tensor(0)\n",
      "prob:  [0.6991738  0.30082622]\n",
      "label:  tensor(0)\n",
      "prob:  [0.67384094 0.3261591 ]\n",
      "label:  tensor(0)\n",
      "prob:  [0.5369048 0.4630952]\n",
      "label:  tensor(0)\n",
      "prob:  [0.64950484 0.3504952 ]\n",
      "label:  tensor(0)\n",
      "prob:  [0.48132476 0.5186752 ]\n",
      "label:  tensor(1)\n",
      "prob:  [0.45784014 0.5421598 ]\n",
      "label:  tensor(1)\n",
      "prob:  [0.58394593 0.41605407]\n",
      "label:  tensor(0)\n",
      "prob:  [0.8786292  0.12137074]\n",
      "label:  tensor(0)\n",
      "prob:  [0.52267045 0.47732955]\n",
      "label:  tensor(0)\n",
      "prob:  [0.52295214 0.47704783]\n",
      "label:  tensor(0)\n",
      "16/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/26 [00:06<00:51,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  [0.32408413 0.6759159 ]\n",
      "label:  tensor(1)\n",
      "prob:  [0.68227404 0.31772596]\n",
      "label:  tensor(0)\n",
      "prob:  [0.56624997 0.43375003]\n",
      "label:  tensor(0)\n",
      "prob:  [0.7076892 0.2923107]\n",
      "label:  tensor(0)\n",
      "prob:  [0.5609128  0.43908724]\n",
      "label:  tensor(0)\n",
      "prob:  [0.7445134  0.25548658]\n",
      "label:  tensor(0)\n",
      "prob:  [0.52526754 0.4747325 ]\n",
      "label:  tensor(0)\n",
      "prob:  [0.44767764 0.55232227]\n",
      "label:  tensor(1)\n",
      "prob:  [0.5929447 0.4070553]\n",
      "label:  tensor(0)\n",
      "prob:  [0.64202887 0.35797116]\n",
      "label:  tensor(0)\n",
      "prob:  [0.4149308 0.5850692]\n",
      "label:  tensor(1)\n",
      "prob:  [0.58232313 0.4176769 ]\n",
      "label:  tensor(0)\n",
      "prob:  [0.82170177 0.17829825]\n",
      "label:  tensor(0)\n",
      "prob:  [0.4752547 0.5247452]\n",
      "label:  tensor(1)\n",
      "prob:  [0.4047317 0.5952683]\n",
      "label:  tensor(1)\n",
      "prob:  [0.60765773 0.3923423 ]\n",
      "label:  tensor(0)\n",
      "prob:  [0.7936013  0.20639874]\n",
      "label:  tensor(0)\n",
      "prob:  [0.49228445 0.5077156 ]\n",
      "label:  tensor(1)\n",
      "prob:  [0.6779369  0.32206318]\n",
      "label:  tensor(0)\n",
      "19/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 4/26 [00:08<00:48,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  [0.1394053 0.8605947]\n",
      "label:  tensor(1)\n",
      "prob:  [0.7417851  0.25821492]\n",
      "label:  tensor(0)\n",
      "prob:  [0.8147286 0.1852714]\n",
      "label:  tensor(0)\n",
      "prob:  [0.93814516 0.06185484]\n",
      "label:  tensor(0)\n",
      "prob:  [0.87101156 0.12898837]\n",
      "label:  tensor(0)\n",
      "prob:  [0.85008234 0.14991772]\n",
      "label:  tensor(0)\n",
      "prob:  [0.69743466 0.30256534]\n",
      "label:  tensor(0)\n",
      "prob:  [0.6315495  0.36845052]\n",
      "label:  tensor(0)\n",
      "prob:  [0.84447163 0.15552837]\n",
      "label:  tensor(0)\n",
      "prob:  [0.68005913 0.31994087]\n",
      "label:  tensor(0)\n",
      "10/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 5/26 [00:11<00:46,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  [0.7949931  0.20500697]\n",
      "label:  tensor(0)\n",
      "prob:  [0.6563752  0.34362474]\n",
      "label:  tensor(0)\n",
      "prob:  [0.5543037  0.44569635]\n",
      "label:  tensor(0)\n",
      "prob:  [0.7290381  0.27096185]\n",
      "label:  tensor(0)\n",
      "prob:  [0.64694655 0.35305342]\n",
      "label:  tensor(0)\n",
      "prob:  [0.68496126 0.31503874]\n",
      "label:  tensor(0)\n",
      "prob:  [0.4110961 0.5889039]\n",
      "label:  tensor(1)\n",
      "prob:  [0.50964665 0.49035335]\n",
      "label:  tensor(0)\n",
      "8/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 6/26 [00:13<00:43,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  [0.40191978 0.5980802 ]\n",
      "label:  tensor(1)\n",
      "prob:  [0.95755553 0.04244443]\n",
      "label:  tensor(0)\n",
      "prob:  [0.5666973 0.4333027]\n",
      "label:  tensor(0)\n",
      "prob:  [0.5314991  0.46850088]\n",
      "label:  tensor(0)\n",
      "prob:  [0.65206444 0.34793556]\n",
      "label:  tensor(0)\n",
      "prob:  [0.8791589  0.12084107]\n",
      "label:  tensor(0)\n",
      "prob:  [0.91028136 0.08971859]\n",
      "label:  tensor(0)\n",
      "prob:  [0.8226838  0.17731619]\n",
      "label:  tensor(0)\n",
      "prob:  [0.44766873 0.55233127]\n",
      "label:  tensor(1)\n",
      "prob:  [0.28506982 0.71493024]\n",
      "label:  tensor(1)\n",
      "prob:  [0.57310593 0.42689407]\n",
      "label:  tensor(0)\n",
      "prob:  [0.82727855 0.1727214 ]\n",
      "label:  tensor(0)\n",
      "prob:  [0.4813129 0.5186871]\n",
      "label:  tensor(1)\n",
      "13/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 7/26 [00:15<00:41,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  [0.86771196 0.132288  ]\n",
      "label:  tensor(0)\n",
      "prob:  [0.5208548  0.47914523]\n",
      "label:  tensor(0)\n",
      "prob:  [0.73144734 0.26855272]\n",
      "label:  tensor(0)\n",
      "prob:  [0.36104724 0.6389528 ]\n",
      "label:  tensor(1)\n",
      "prob:  [0.58330536 0.41669467]\n",
      "label:  tensor(0)\n",
      "prob:  [0.66145957 0.33854038]\n",
      "label:  tensor(0)\n",
      "prob:  [0.77652997 0.22347006]\n",
      "label:  tensor(0)\n",
      "prob:  [0.78360057 0.21639939]\n",
      "label:  tensor(0)\n",
      "prob:  [0.7954887  0.20451127]\n",
      "label:  tensor(0)\n",
      "prob:  [0.52092    0.47908002]\n",
      "label:  tensor(0)\n",
      "prob:  [0.69896156 0.3010385 ]\n",
      "label:  tensor(0)\n",
      "prob:  [0.3664976  0.63350236]\n",
      "label:  tensor(1)\n",
      "12/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 7/26 [00:16<00:44,  2.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 135\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    134\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(test_loader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader))\n\u001b[1;32m--> 135\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_correct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[22], line 99\u001b[0m, in \u001b[0;36mimage_title_dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# Load image and preprocess\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx])\n\u001b[1;32m---> 99\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[0;32m    100\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\PIL\\Image.py:2345\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2342\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mresize(size, resample, box)\n\u001b[0;32m   2343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m-> 2345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reducing_gap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resample \u001b[38;5;241m!=\u001b[39m Resampling\u001b[38;5;241m.\u001b[39mNEAREST:\n\u001b[0;32m   2348\u001b[0m     factor_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m reducing_gap) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pdb\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from plip import PLIP\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Set paths\n",
    "data_path = 'Thyroid_Cancer_TAN&NOH_file.csv' #'/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'\n",
    "base_image_path = '' # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# class CLIPBinaryClassifier(nn.Module):\n",
    "#     def __init__(self, clip_model):\n",
    "#         super(CLIPBinaryClassifier, self).__init__()\n",
    "#         self.clip_model = clip_model\n",
    "        \n",
    "#         hidden_size = clip_model.text_model.config.hidden_size  \n",
    "              \n",
    "#         # Modify the final layer to produce 2 logits for binary classification\n",
    "#         self.classifier = nn.Linear(hidden_size, 2)  # 2 output classes\n",
    "        \n",
    "#     def forward(self, input_ids, pixel_values):\n",
    "#         # Get the outputs from the CLIP model\n",
    "#         outputs = self.clip_model(input_ids=input_ids, pixel_values=pixel_values)\n",
    "        \n",
    "#         # Get the logits per image (for classification)\n",
    "#         logits_per_image = outputs.logits_per_image\n",
    "        \n",
    "#         # Pass logits through the classifier to reduce the number of classes to 2\n",
    "#         logits_per_image = self.classifier(logits_per_image)  # [batch_size, 2]\n",
    "        \n",
    "#         # Return the logits for both image and text\n",
    "#         return logits_per_image, outputs.logits_per_text\n",
    "\n",
    "# Load the pretrained CLIP model\n",
    "model = CLIPModel.from_pretrained(\"vinid/plip\")\n",
    "# Wrap the model with the binary classifier\n",
    "# model = CLIPBinaryClassifier(clip_model)\n",
    "processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n",
    " \n",
    "class image_title_dataset():\n",
    "    def __init__(self, dataframe, base_path):\n",
    "        self.image_paths = [os.path.join(base_path, row['image_path'].replace('\\\\', '/')) for _, row in dataframe.iterrows()]\n",
    "        # self.labels = [clip.tokenize('benign') if row['Surgery diagnosis in number'] == 0 else clip.tokenize('cancer') for _, row in dataframe.iterrows()]\n",
    "        self.labels = [ row['Surgery diagnosis in number']  for _, row in dataframe.iterrows()]\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)), \n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and preprocess\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        image = self.preprocess(image)     \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Load data and prepare for training\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['Surgery diagnosis in number'])  # Drop rows with NaN labels\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = image_title_dataset(train_df, base_image_path)\n",
    "test_dataset = image_title_dataset(test_df, base_image_path)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "# the text classes the model will be using\n",
    "text = clip.tokenize([\"benign\", \"cancer\"]).to(device)\n",
    "\n",
    "# Training loop -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    pbar = tqdm(test_loader, total=len(test_loader))\n",
    "    for batch in pbar:\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            num_correct = 0\n",
    "            images,labels = batch \n",
    "            images= images.to(device)\n",
    "            labels= labels.to(device).long()\n",
    "            # Encode image and text\n",
    "            # image_features = model.encode_image(image)\n",
    "            # text_features = model.encode_text(text)\n",
    "            \n",
    "            # Calculate similarity scores between image and text\n",
    "            output = model(text, images)\n",
    "            logits_per_image = output.logits_per_image\n",
    "            logits_per_text = output.logits_per_text\n",
    "            \n",
    "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "            for i, prob in enumerate(probs):\n",
    "                if prob.argmax() == labels[i]:\n",
    "                    num_correct += 1\n",
    "                    print(\"prob: \", prob)\n",
    "                    print(\"label: \", labels[i])\n",
    "\n",
    "            print(f\"{num_correct}/{len(labels)}\")\n",
    "    \n",
    "    pbar = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images,labels = batch \n",
    "        images= images.to(device)\n",
    "        labels= labels.to(device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(text, pixel_values=images)\n",
    "\n",
    "        logits_per_image = output.logits_per_image\n",
    "        logits_per_text = output.logits_per_text\n",
    "        \n",
    "        # image_predictions = torch.argmax(logits_per_image, dim=1)\n",
    "\n",
    "        # Compute loss\n",
    "        total_loss = (loss_img(logits_per_image, labels))\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "# Validation loop -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    pbar = tqdm(test_loader, total=len(test_loader))\n",
    "    for batch in pbar:\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            images,labels = batch \n",
    "            images= images.to(device)\n",
    "            labels= labels.to(device).long()\n",
    "            # Encode image and text\n",
    "            # image_features = model.encode_image(image)\n",
    "            # text_features = model.encode_text(text)\n",
    "            \n",
    "            print(labels)\n",
    "            # Calculate similarity scores between image and text\n",
    "            logits_per_image, logits_per_text = model(images, text)\n",
    "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "            \n",
    "            print(probs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sasp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
