{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install imbalanced-learn\n",
    "\n",
    "# commented out pips\n",
    "# replaced the data_path and base_image_path, tan_directory, df1, df2, final_df.to_csv\n",
    "# Please make sure to only add this file, and not the other green ones. I don't want to brake stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'efficientnet_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm  \n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mefficientnet_pytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mEfficinet\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Set paths\u001b[39;00m\n\u001b[0;32m     10\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_NOH.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# '/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv'\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'efficientnet_pytorch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, UnidentifiedImageError \n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "import efficientnet_pytorch as Efficinet\n",
    "\n",
    "# Set paths\n",
    "data_path = \"data_NOH.csv\"  # '/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv'\n",
    "base_image_path = \"\" # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(clip.__file__)\n",
    "\n",
    "# Load the CLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to process a single image and text entry through CLIP\n",
    "def process_image_and_text(image_path, text, model, preprocess):\n",
    "    # Open image with error handling for corrupt files\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Error opening image {image_path}: {e}. Skipping.\")\n",
    "        return None, None  # Return None to indicate failure\n",
    "\n",
    "    # Process text\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    return image_features.cpu().numpy(), text_features.cpu().numpy()\n",
    "\n",
    "# Initialize the list to keep track of successfully processed image paths\n",
    "processed_image_paths = []\n",
    "\n",
    "# Iterate over the dataframe and compute embeddings with progress bar\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "# Wrap the iteration in tqdm for progress tracking\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\", unit=\"row\"):\n",
    "    img_path = os.path.join(base_image_path, row['image_path'].replace('\\\\', '/'))\n",
    "    diagnosis_text = row['Surgery diagnosis']\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # Process image and text\n",
    "        img_embed, txt_embed = process_image_and_text(img_path, diagnosis_text, model, preprocess)\n",
    "        \n",
    "        if img_embed is not None and txt_embed is not None:\n",
    "            image_embeddings.append(img_embed)\n",
    "            text_embeddings.append(txt_embed)\n",
    "            # Append the successful image path to the list\n",
    "            processed_image_paths.append(row['image_path'])\n",
    "    else:\n",
    "        print(f\"Image {img_path} does not exist. Skipping.\")\n",
    "\n",
    "# Convert embeddings to arrays and save for later use\n",
    "if image_embeddings:\n",
    "    image_embeddings = torch.cat([torch.tensor(x) for x in image_embeddings], dim=0)\n",
    "    torch.save(image_embeddings, 'image_embeddings.pt')\n",
    "\n",
    "if text_embeddings:\n",
    "    text_embeddings = torch.cat([torch.tensor(x) for x in text_embeddings], dim=0)\n",
    "    torch.save(text_embeddings, 'text_embeddings.pt')\n",
    "\n",
    "print(\"Embeddings saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "# Check the shape of the image embeddings\n",
    "print(image_embeddings.shape)\n",
    "# Check the number of image embeddings generated\n",
    "print(len(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the embeddings\n",
    "image_embeddings = torch.load('image_embeddings.pt')\n",
    "text_embeddings = torch.load('text_embeddings.pt')\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_embeddings, labels, test_size=0.25, random_state=42, stratify=labels)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model with class weight and regularization\n",
    "clf = LogisticRegression(class_weight='balanced', C=0.8)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "print(\"ROC-AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the TAN directory\n",
    "tan_directory = \"TAN\" # '/home/iambrink/NOH_Thyroid_Cancer_Data/TAN'  # Update this path to your TAN folder\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(tan_directory):\n",
    "    if filename.startswith(\"TAN\"):  # Check if the filename starts with \"TAN\"\n",
    "        # Remove the \"TAN\" prefix\n",
    "        new_filename = filename.replace(\"TAN\", \"\", 1)  # Remove \"TAN\" only once\n",
    "        new_filename = new_filename.lstrip(\"\\\\\")  # Remove any leading backslashes if necessary\n",
    "        \n",
    "        # Create full paths for renaming\n",
    "        old_file = os.path.join(tan_directory, filename)\n",
    "        new_file = os.path.join(tan_directory, new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(old_file, new_file)\n",
    "\n",
    "print(\"Files renamed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 =  pd.read_csv('data_NOH.csv') # pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('data_TAN.csv') # pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_TAN_V2.csv')\n",
    "\n",
    "# Select relevant columns from df1\n",
    "columns_df1 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',\n",
    "    'image_path',  # Keep the image_path from df1\n",
    "    'Surgery diagnosis'\n",
    "]\n",
    "\n",
    "# Select relevant columns from df2\n",
    "columns_df2 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',  # Include relevant columns from df2\n",
    "    'image_path',                    # Keep the image_path from df2\n",
    "    'Surgery diagnosis'\n",
    "]\n",
    "\n",
    "# Create DataFrames with only the selected columns\n",
    "df1_selected = df1[columns_df1]\n",
    "df2_selected = df2[columns_df2]\n",
    "\n",
    "# Get the maximum patient number from df1\n",
    "max_patient_number = df1_selected['Patient #'].max()\n",
    "\n",
    "# Update patient numbers in df2 by adding max_patient_number\n",
    "df2_selected['Patient #'] = df2_selected['Patient #'] + max_patient_number\n",
    "\n",
    "# Concatenate the two DataFrames vertically\n",
    "final_df = pd.concat([df1_selected, df2_selected], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "# final_df.to_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "final_df.to_csv('Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "\n",
    "print(\"Super CSV file with adjusted Patient # has been created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 = pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_TAN_V2.csv')\n",
    "\n",
    "# Select relevant columns from df1\n",
    "columns_df1 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',\n",
    "    'image_path'  # Keep the image_path from df1\n",
    "]\n",
    "\n",
    "# Select relevant columns from df2\n",
    "columns_df2 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',  # Include relevant columns from df2\n",
    "    'image_path'                    # Keep the image_path from df2\n",
    "]\n",
    "\n",
    "# Create DataFrames with only the selected columns\n",
    "df1_selected = df1[columns_df1]\n",
    "df2_selected = df2[columns_df2]\n",
    "\n",
    "# Get the maximum patient number from df1\n",
    "max_patient_number = df1_selected['Patient #'].max()\n",
    "\n",
    "# Update patient numbers in df2 by adding max_patient_number\n",
    "df2_selected['Patient #'] = df2_selected['Patient #'] + max_patient_number\n",
    "\n",
    "# Adjust image_path in df2_selected to format it correctly\n",
    "df2_selected['image_path'] = df2_selected['image_path'].str.replace(r'TAN/', '', regex=True)  # Remove 'TAN/' prefix\n",
    "\n",
    "# Extract the numeric part; fill NaN values with empty strings to avoid TypeError\n",
    "df2_selected['image_number'] = df2_selected['image_path'].str.extract(r'(\\d+)')  # Extract the numeric part\n",
    "\n",
    "# Construct the new image_path without duplicating 'TAN'\n",
    "df2_selected['image_path'] = 'TAN\\\\' + df2_selected['image_number'].fillna('Unknown') + '\\\\' + df2_selected['image_path'].str.replace(r'TAN\\d*\\\\', '', regex=True)\n",
    "\n",
    "# Drop the image_number column\n",
    "df2_selected = df2_selected.drop(columns=['image_number'])\n",
    "\n",
    "# Concatenate the two DataFrames vertically\n",
    "final_df = pd.concat([df1_selected, df2_selected], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "final_df.to_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "\n",
    "print(\"Super CSV file with adjusted Patient # and image_path has been created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, UnidentifiedImageError \n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "\n",
    "# Set paths\n",
    "data_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'  # Update to your super CSV file\n",
    "base_image_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/'  # Adjust based on where your images are located\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Load the CLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to process a single image and text entry through CLIP\n",
    "def process_image_and_text(image_path, text, model, preprocess):\n",
    "    # Open image with error handling for corrupt files\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Error opening image {image_path}: {e}. Skipping.\")\n",
    "        return None, None  # Return None to indicate failure\n",
    "\n",
    "    # Process text\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    return image_features.cpu().numpy(), text_features.cpu().numpy()\n",
    "\n",
    "# Initialize the list to keep track of successfully processed image paths\n",
    "processed_image_paths = []\n",
    "\n",
    "# Iterate over the dataframe and compute embeddings with progress bar\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "# Wrap the iteration in tqdm for progress tracking\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\", unit=\"row\"):\n",
    "    # Construct the full image path\n",
    "    img_path = os.path.join(base_image_path, row['image_path'].replace('\\\\', '/'))\n",
    "    diagnosis_text = str(row['Surgery diagnosis in number'])  # Use surgery diagnosis in number as text\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # Process image and text\n",
    "        img_embed, txt_embed = process_image_and_text(img_path, diagnosis_text, model, preprocess)\n",
    "        \n",
    "        if img_embed is not None and txt_embed is not None:\n",
    "            image_embeddings.append(img_embed)\n",
    "            text_embeddings.append(txt_embed)\n",
    "            # Append the successful image path to the list\n",
    "            processed_image_paths.append(row['image_path'])\n",
    "    else:\n",
    "        print(f\"Image {img_path} does not exist. Skipping.\")\n",
    "\n",
    "# Convert embeddings to arrays and save for later use\n",
    "if image_embeddings:\n",
    "    image_embeddings = torch.cat([torch.tensor(x) for x in image_embeddings], dim=0)\n",
    "    torch.save(image_embeddings, 'image_embeddings.pt')\n",
    "\n",
    "if text_embeddings:\n",
    "    text_embeddings = torch.cat([torch.tensor(x) for x in text_embeddings], dim=0)\n",
    "    torch.save(text_embeddings, 'text_embeddings.pt')\n",
    "\n",
    "print(\"Embeddings saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "# Check the shape of the image embeddings\n",
    "print(image_embeddings.shape)\n",
    "# Check the number of image embeddings generated\n",
    "print(len(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the embeddings\n",
    "image_embeddings = torch.load('image_embeddings.pt')\n",
    "text_embeddings = torch.load('text_embeddings.pt')\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have your image_embeddings and labels defined\n",
    "# Filter out rows where labels are NaN\n",
    "valid_indices = ~np.isnan(labels)\n",
    "image_embeddings_filtered = image_embeddings[valid_indices]\n",
    "labels_filtered = labels[valid_indices]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_embeddings_filtered, labels_filtered, test_size=0.25, random_state=42, stratify=labels_filtered)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression(class_weight='balanced', C=0.8)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Save the model\n",
    "dump(clf, 'logistic_regression_model.joblib')  # Using joblib\n",
    "# or\n",
    "# with open('logistic_regression_model.pkl', 'wb') as file:  # Using pickle\n",
    "#     pickle.dump(clf, file)\n",
    "\n",
    "# Load the model\n",
    "clf_loaded = load('logistic_regression_model.joblib')  # Using joblib\n",
    "# or\n",
    "# with open('logistic_regression_model.pkl', 'rb') as file:  # Using pickle\n",
    "#     clf_loaded = pickle.load(file)\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "y_pred = clf_loaded.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_test, clf_loaded.predict_proba(X_test)[:, 1])\n",
    "print(\"ROC-AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # Assuming RGB images\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, 1)  # Binary classification (output one value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 56 * 56)  # Flatten the tensor\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match CNN input size\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pdb\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from plip import PLIP\n",
    "import numpy as np\n",
    "\n",
    "plip = PLIP('vinid/plip')\n",
    "\n",
    "plip_model = CLIPModel.from_pretrained(\"vinid/plip\")\n",
    "plip_processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n",
    "plip_texts = np.unique(df['Surgery diagnosis']).tolist()\n",
    "\n",
    "# Set paths\n",
    "data_path = 'Thyroid_Cancer_TAN&NOH_file.csv' #'/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'\n",
    "base_image_path = '' # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match CNN input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create your own Dataset class to load images and labels\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, base_path, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.base_path, self.dataframe.iloc[idx]['image_path'].replace('\\\\', '/'))\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx]['Surgery diagnosis in number']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load data and prepare for training\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['Surgery diagnosis in number'])  # Drop rows with NaN labels\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = CustomDataset(train_df, base_image_path, transform=transform)\n",
    "test_dataset = CustomDataset(test_df, base_image_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "\n",
    "# Training loop with tqdm\n",
    "num_epochs = 200\n",
    "best_val_acc = 0.0  # To keep track of the best validation accuracy\n",
    "\n",
    "# Heat map using captum\n",
    "\n",
    "heat_map_data_iter = iter(test_loader)\n",
    "heat_map_images, heat_map_labels = next(heat_map_data_iter)\n",
    "test_img = heat_map_images[0]\n",
    "test_label = heat_map_labels[0]\n",
    "\n",
    "output = model(test_img)\n",
    "pred_label_idx = output.argmax(dim=1).item()\n",
    "\n",
    "occlusion = Occlusion(model)\n",
    "\n",
    "# Compute the attributions\n",
    "attributions_occ = occlusion.attribute(\n",
    "    test_img,\n",
    "    target=pred_label_idx,\n",
    "    strides=(16, 16),  # Spatial dimensions only\n",
    "    sliding_window_shapes=(32, 32),  # Spatial dimensions only\n",
    "    baselines=0\n",
    ")\n",
    "\n",
    "# Overlay the attribution on top of the original image with the chosen colormap\n",
    "_ = viz.visualize_image_attr(\n",
    "    np.expand_dims(attributions_occ.squeeze().cpu().detach().numpy(), axis=-1),\n",
    "    np.transpose(test_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "    method=\"blended_heat_map\",\n",
    "    sign=\"all\",\n",
    "    show_colorbar=True,\n",
    "    title=\"Original Image with Colored Attribution Overlay\",\n",
    "    fig_size=(8, 8),\n",
    "    use_pyplot=True,\n",
    "    alpha_overlay=0.5  # Control the transparency of the overlay (0 is fully transparent, 1 is fully opaque)\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.float().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update training metrics\n",
    "            train_loss += loss.item()\n",
    "            predictions = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)  # Increment the progress bar\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = correct_predictions / len(train_dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels.float().to(device))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predictions = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "            val_correct_predictions += (predictions == labels.float().to(device)).sum().item()\n",
    "\n",
    "            val_labels.extend(labels.numpy())\n",
    "            val_preds.extend(predictions.cpu().numpy())\n",
    "            \n",
    "            # add in plip guesses\n",
    "            for image in images:\n",
    "                plip_inputs = plip_processor(text=plip_texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "                plip_outputs = plip_model(**plip_inputs)\n",
    "                plip_logits_per_image = plip_outputs.logits_per_image  # this is the image-text similarity score\n",
    "                plip_probs = plip_logits_per_image.softmax(dim=1) \n",
    "                plip_response = plip_texts[np.argmax(plip_probs.detach().numpy() )]\n",
    "                print(plip_response)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(test_loader)\n",
    "    val_acc = val_correct_predictions / len(test_dataset)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Heat map using captum\n",
    "\n",
    "    heat_map_data_iter = iter(test_loader)\n",
    "    heat_map_images, heat_map_labels = next(heat_map_data_iter)\n",
    "    test_img = heat_map_images[0]\n",
    "    test_label = heat_map_labels[0]\n",
    "\n",
    "    output = model(test_img)\n",
    "    pred_label_idx = output.argmax(dim=1).item()\n",
    "\n",
    "    occlusion = Occlusion(model)\n",
    "\n",
    "    # Compute the attributions\n",
    "    attributions_occ = occlusion.attribute(\n",
    "        test_img,\n",
    "        target=pred_label_idx,\n",
    "        strides=(16, 16),  # Spatial dimensions only\n",
    "        sliding_window_shapes=(32, 32),  # Spatial dimensions only\n",
    "        baselines=0\n",
    "    )\n",
    "\n",
    "    # Overlay the attribution on top of the original image with the chosen colormap\n",
    "    _ = viz.visualize_image_attr(\n",
    "        np.expand_dims(attributions_occ.squeeze().cpu().detach().numpy(), axis=-1),\n",
    "        np.transpose(test_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "        method=\"blended_heat_map\",\n",
    "        sign=\"all\",\n",
    "        show_colorbar=True,\n",
    "        title=\"Original Image with Colored Attribution Overlay\",\n",
    "        fig_size=(8, 8),\n",
    "        use_pyplot=True,\n",
    "        alpha_overlay=0.5  # Control the transparency of the overlay (0 is fully transparent, 1 is fully opaque)\n",
    "    )\n",
    "\n",
    "    # Save the model if the validation accuracy improves\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'super_CNN_Model.pth')\n",
    "        print(\"Model saved with best validation accuracy!\")\n",
    "\n",
    "# Final evaluation\n",
    "model.load_state_dict(torch.load('super_CNN_Model.pth'))  # Load the best model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        predictions = torch.round(torch.sigmoid(outputs.squeeze()))  # Convert probabilities to binary predictions\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_true, y_pred)\n",
    "print(\"ROC-AUC Score:\", auc_score)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "#100 ep - 325 mins, best val acc 88.18% #200 ep - 682 min, best val acc 88.41%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the model and evaluate\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleCNN\u001b[49m()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuper_CNN_Model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# Load the best model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SimpleCNN' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model and evaluate\n",
    "model = SimpleCNN().to(device)\n",
    "model.load_state_dict(torch.load('super_CNN_Model.pth'))  # Load the best model\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Example inference on a single image\n",
    "def predict_image(image_path, model, transform, device):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title('Input Image')\n",
    "    plt.show()\n",
    "\n",
    "    # Transform and add batch dimension\n",
    "    image_transformed = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image_transformed)\n",
    "        probability = torch.sigmoid(output).item()  # Get the probability (confidence between 0-1)\n",
    "        \n",
    "        # Classify based on the probability threshold (0.5 by default)\n",
    "        if probability >= 0.5:\n",
    "            prediction = 'Cancer'\n",
    "            confidence = probability\n",
    "        else:\n",
    "            prediction = 'Non-Cancer'\n",
    "            confidence = 1 - probability  # Confidence for non-cancer is 1 - probability\n",
    "            \n",
    "    return prediction, confidence\n",
    "\n",
    "# Use the prediction function\n",
    "sample_image_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/TAN/001/IMG_20220623_134910.jpg'  # Replace with actual path\n",
    "predicted_class, confidence = predict_image(sample_image_path, model, transform, device)\n",
    "print(f'Predicted Class: {predicted_class}, Confidence: {confidence:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the plip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available.\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Loss: 0.5232: 100%|██████████| 77/77 [04:47<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:55<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.64      0.75       365\n",
      "           1       0.76      0.95      0.85       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.84      0.79      0.80       812\n",
      "weighted avg       0.83      0.81      0.80       812\n",
      "\n",
      "AUC: 0.8189267874107445\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1.0634: 100%|██████████| 77/77 [04:35<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:56<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76       365\n",
      "           1       0.78      0.93      0.85       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.80      0.81       812\n",
      "weighted avg       0.83      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8210995678955594\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000, Loss: 0.2803: 100%|██████████| 77/77 [5:36:55<00:00, 262.54s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:56<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.67      0.77       365\n",
      "           1       0.78      0.94      0.85       447\n",
      "\n",
      "    accuracy                           0.82       812\n",
      "   macro avg       0.84      0.81      0.81       812\n",
      "weighted avg       0.83      0.82      0.81       812\n",
      "\n",
      "AUC: 0.8064723728969384\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000, Loss: 0.9668: 100%|██████████| 77/77 [04:31<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:56<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.51      0.66       365\n",
      "           1       0.71      0.97      0.82       447\n",
      "\n",
      "    accuracy                           0.76       812\n",
      "   macro avg       0.82      0.74      0.74       812\n",
      "weighted avg       0.81      0.76      0.75       812\n",
      "\n",
      "AUC: 0.809248873770341\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000, Loss: 0.0090: 100%|██████████| 77/77 [04:22<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:55<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.68      0.76       365\n",
      "           1       0.78      0.92      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.82      0.80      0.80       812\n",
      "weighted avg       0.82      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8366890380313199\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000, Loss: 0.5372: 100%|██████████| 77/77 [04:23<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:54<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.69      0.77       365\n",
      "           1       0.78      0.90      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.82      0.80      0.80       812\n",
      "weighted avg       0.82      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8203793938279549\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000, Loss: 0.0943: 100%|██████████| 77/77 [04:22<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:55<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.66      0.76       365\n",
      "           1       0.77      0.94      0.85       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.84      0.80      0.80       812\n",
      "weighted avg       0.83      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8204222978149613\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000, Loss: 0.2499: 100%|██████████| 77/77 [04:23<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:54<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.36      0.52       365\n",
      "           1       0.65      0.98      0.78       447\n",
      "\n",
      "    accuracy                           0.70       812\n",
      "   macro avg       0.79      0.67      0.65       812\n",
      "weighted avg       0.78      0.70      0.67       812\n",
      "\n",
      "AUC: 0.8210873096135576\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000, Loss: 0.2968: 100%|██████████| 77/77 [04:23<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:56<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.66      0.76       365\n",
      "           1       0.77      0.93      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.80      0.80       812\n",
      "weighted avg       0.83      0.81      0.81       812\n",
      "\n",
      "AUC: 0.819784867150869\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000, Loss: 0.4074: 100%|██████████| 77/77 [04:24<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:55<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.68      0.77       365\n",
      "           1       0.78      0.91      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.82      0.80      0.80       812\n",
      "weighted avg       0.82      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8356286966381661\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000, Loss: 0.1075: 100%|██████████| 77/77 [04:47<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:55<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.68      0.76       365\n",
      "           1       0.78      0.92      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.80      0.80       812\n",
      "weighted avg       0.82      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8216542551561399\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000, Loss: 0.7678: 100%|██████████| 77/77 [04:24<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:55<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.68      0.76       365\n",
      "           1       0.78      0.92      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.80      0.80       812\n",
      "weighted avg       0.82      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8432227023382673\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000, Loss: 0.7235: 100%|██████████| 77/77 [04:25<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:54<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.69      0.77       365\n",
      "           1       0.78      0.91      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.82      0.80      0.80       812\n",
      "weighted avg       0.82      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8399252244797891\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000, Loss: 1.2694: 100%|██████████| 77/77 [04:32<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:54<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.67      0.76       365\n",
      "           1       0.77      0.92      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.82      0.80      0.80       812\n",
      "weighted avg       0.82      0.81      0.80       812\n",
      "\n",
      "AUC: 0.8409855658729429\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000, Loss: 0.8167: 100%|██████████| 77/77 [04:36<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:54<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.65      0.76       365\n",
      "           1       0.77      0.94      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.79      0.80       812\n",
      "weighted avg       0.82      0.81      0.80       812\n",
      "\n",
      "AUC: 0.8534399803867488\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000, Loss: 1.3907: 100%|██████████| 77/77 [04:49<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:55<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.65      0.76       365\n",
      "           1       0.77      0.94      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.79      0.80       812\n",
      "weighted avg       0.82      0.81      0.80       812\n",
      "\n",
      "AUC: 0.8517115626244982\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000, Loss: 0.4424: 100%|██████████| 77/77 [05:01<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:54<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.68      0.77       365\n",
      "           1       0.78      0.93      0.85       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.80      0.81       812\n",
      "weighted avg       0.82      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8591584689405779\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000, Loss: 0.6564: 100%|██████████| 77/77 [05:07<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:54<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.58      0.71       365\n",
      "           1       0.74      0.95      0.83       447\n",
      "\n",
      "    accuracy                           0.79       812\n",
      "   macro avg       0.82      0.77      0.77       812\n",
      "weighted avg       0.81      0.79      0.78       812\n",
      "\n",
      "AUC: 0.8582820017774508\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000, Loss: 0.6191: 100%|██████████| 77/77 [05:05<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:55<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76       365\n",
      "           1       0.77      0.93      0.85       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.80      0.80       812\n",
      "weighted avg       0.83      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8608378535748216\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000, Loss: 0.2821: 100%|██████████| 77/77 [05:07<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:56<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.67      0.76       365\n",
      "           1       0.77      0.93      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.80      0.80       812\n",
      "weighted avg       0.82      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8584260365909717\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000, Loss: 1.2247: 100%|██████████| 77/77 [05:06<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:56<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.65      0.76       365\n",
      "           1       0.77      0.94      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.79      0.80       812\n",
      "weighted avg       0.82      0.81      0.80       812\n",
      "\n",
      "AUC: 0.8625969170420765\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000, Loss: 0.2282: 100%|██████████| 77/77 [05:09<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:56<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.77       365\n",
      "           1       0.82      0.79      0.81       447\n",
      "\n",
      "    accuracy                           0.79       812\n",
      "   macro avg       0.79      0.79      0.79       812\n",
      "weighted avg       0.79      0.79      0.79       812\n",
      "\n",
      "AUC: 0.862903374092121\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000, Loss: 0.1444: 100%|██████████| 77/77 [05:09<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:56<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.68      0.77       365\n",
      "           1       0.78      0.93      0.85       447\n",
      "\n",
      "    accuracy                           0.82       812\n",
      "   macro avg       0.83      0.80      0.81       812\n",
      "weighted avg       0.83      0.82      0.81       812\n",
      "\n",
      "AUC: 0.859483313413625\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000, Loss: 0.4639: 100%|██████████| 77/77 [05:11<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:56<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.68      0.76       365\n",
      "           1       0.78      0.92      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.80      0.80       812\n",
      "weighted avg       0.82      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8662621433606079\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000, Loss: 0.2335: 100%|██████████| 77/77 [05:13<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:56<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.71      0.78       365\n",
      "           1       0.79      0.91      0.85       447\n",
      "\n",
      "    accuracy                           0.82       812\n",
      "   macro avg       0.83      0.81      0.81       812\n",
      "weighted avg       0.82      0.82      0.81       812\n",
      "\n",
      "AUC: 0.8651834145444516\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000, Loss: 0.2050: 100%|██████████| 77/77 [05:27<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:57<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76       365\n",
      "           1       0.77      0.94      0.85       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.80      0.80       812\n",
      "weighted avg       0.83      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8656185835555147\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000, Loss: 1.2156: 100%|██████████| 77/77 [05:48<00:00,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:57<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.67      0.76       365\n",
      "           1       0.77      0.93      0.84       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.80      0.80       812\n",
      "weighted avg       0.82      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8663111764886152\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000, Loss: 0.3091: 100%|██████████| 77/77 [06:28<00:00,  5.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:58<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.61      0.73       365\n",
      "           1       0.75      0.94      0.83       447\n",
      "\n",
      "    accuracy                           0.79       812\n",
      "   macro avg       0.82      0.78      0.78       812\n",
      "weighted avg       0.82      0.79      0.79       812\n",
      "\n",
      "AUC: 0.8645275964573564\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000, Loss: 0.9554: 100%|██████████| 77/77 [07:52<00:00,  6.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:59<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.75      0.79       365\n",
      "           1       0.81      0.88      0.85       447\n",
      "\n",
      "    accuracy                           0.82       812\n",
      "   macro avg       0.83      0.82      0.82       812\n",
      "weighted avg       0.82      0.82      0.82       812\n",
      "\n",
      "AUC: 0.866531825564647\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000, Loss: 1.0119: 100%|██████████| 77/77 [09:16<00:00,  7.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:02<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.71      0.78       365\n",
      "           1       0.79      0.91      0.84       447\n",
      "\n",
      "    accuracy                           0.82       812\n",
      "   macro avg       0.83      0.81      0.81       812\n",
      "weighted avg       0.82      0.82      0.81       812\n",
      "\n",
      "AUC: 0.8627746621311023\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000, Loss: 0.2362: 100%|██████████| 77/77 [10:43<00:00,  8.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:13<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.69      0.78       365\n",
      "           1       0.79      0.94      0.86       447\n",
      "\n",
      "    accuracy                           0.83       812\n",
      "   macro avg       0.84      0.81      0.82       812\n",
      "weighted avg       0.84      0.83      0.82       812\n",
      "\n",
      "AUC: 0.8638472618062578\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000, Loss: 0.1568: 100%|██████████| 77/77 [13:00<00:00, 10.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:42<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.79       365\n",
      "           1       0.84      0.81      0.82       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.81      0.81      0.81       812\n",
      "weighted avg       0.81      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8706260917532408\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000, Loss: 1.0747: 100%|██████████| 77/77 [15:05<00:00, 11.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [02:18<00:00,  5.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76       365\n",
      "           1       0.78      0.93      0.85       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.83      0.80      0.81       812\n",
      "weighted avg       0.83      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8688578345744845\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000, Loss: 0.1044: 100%|██████████| 77/77 [16:55<00:00, 13.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [02:56<00:00,  6.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79       365\n",
      "           1       0.82      0.85      0.83       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.81      0.81      0.81       812\n",
      "weighted avg       0.81      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8690692899390149\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000, Loss: 0.4736: 100%|██████████| 77/77 [18:38<00:00, 14.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [03:27<00:00,  7.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.93      0.72       365\n",
      "           1       0.89      0.45      0.60       447\n",
      "\n",
      "    accuracy                           0.67       812\n",
      "   macro avg       0.74      0.69      0.66       812\n",
      "weighted avg       0.75      0.67      0.65       812\n",
      "\n",
      "AUC: 0.863890165793264\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000, Loss: 0.1496: 100%|██████████| 77/77 [19:42<00:00, 15.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [03:46<00:00,  8.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.78       365\n",
      "           1       0.82      0.81      0.82       447\n",
      "\n",
      "    accuracy                           0.80       812\n",
      "   macro avg       0.80      0.80      0.80       812\n",
      "weighted avg       0.80      0.80      0.80       812\n",
      "\n",
      "AUC: 0.866697312371671\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000, Loss: 0.0617: 100%|██████████| 77/77 [19:37<00:00, 15.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [04:01<00:00,  9.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78       365\n",
      "           1       0.81      0.85      0.83       447\n",
      "\n",
      "    accuracy                           0.81       812\n",
      "   macro avg       0.81      0.80      0.81       812\n",
      "weighted avg       0.81      0.81      0.81       812\n",
      "\n",
      "AUC: 0.8680886273788728\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000, Loss: 1.4085: 100%|██████████| 77/77 [19:44<00:00, 15.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 24/26 [03:56<00:19,  9.86s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pdb\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from plip import PLIP\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Set paths\n",
    "data_path = 'Thyroid_Cancer_TAN&NOH_file.csv' #'/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'\n",
    "base_image_path = '' # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# class CLIPBinaryClassifier(nn.Module):\n",
    "#     def __init__(self, clip_model):\n",
    "#         super(CLIPBinaryClassifier, self).__init__()\n",
    "#         self.clip_model = clip_model\n",
    "        \n",
    "#         hidden_size = clip_model.text_model.config.hidden_size  \n",
    "              \n",
    "#         # Modify the final layer to produce 2 logits for binary classification\n",
    "#         self.classifier = nn.Linear(hidden_size, 2)  # 2 output classes\n",
    "        \n",
    "#     def forward(self, input_ids, pixel_values):\n",
    "#         # Get the outputs from the CLIP model\n",
    "#         outputs = self.clip_model(input_ids=input_ids, pixel_values=pixel_values)\n",
    "        \n",
    "#         # Get the logits per image (for classification)\n",
    "#         logits_per_image = outputs.logits_per_image\n",
    "        \n",
    "#         # Pass logits through the classifier to reduce the number of classes to 2\n",
    "#         logits_per_image = self.classifier(logits_per_image)  # [batch_size, 2]\n",
    "        \n",
    "#         # Return the logits for both image and text\n",
    "#         return logits_per_image, outputs.logits_per_text\n",
    "\n",
    "# Load the pretrained CLIP model\n",
    "model = CLIPModel.from_pretrained(\"vinid/plip\")\n",
    "# Wrap the model with the binary classifier\n",
    "# model = CLIPBinaryClassifier(clip_model)\n",
    "processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n",
    " \n",
    "class image_title_dataset():\n",
    "    def __init__(self, dataframe, base_path):\n",
    "        self.image_paths = [os.path.join(base_path, row['image_path'].replace('\\\\', '/')) for _, row in dataframe.iterrows()]\n",
    "        # self.labels = [clip.tokenize('benign') if row['Surgery diagnosis in number'] == 0 else clip.tokenize('cancer') for _, row in dataframe.iterrows()]\n",
    "        self.labels = [ row['Surgery diagnosis in number']  for _, row in dataframe.iterrows()]\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)), \n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and preprocess\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        image = self.preprocess(image)     \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Load data and prepare for training\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['Surgery diagnosis in number'])  # Drop rows with NaN labels\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = image_title_dataset(train_df, base_image_path)\n",
    "test_dataset = image_title_dataset(test_df, base_image_path)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "# the text classes the model will be using\n",
    "text = clip.tokenize([\"benign\", \"cancer\"]).to(device)\n",
    "\n",
    "# Training loop -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print(\"Training\")\n",
    "    pbar = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images,labels = batch \n",
    "        images= images.to(device)\n",
    "        labels= labels.to(device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(text, pixel_values=images)\n",
    "\n",
    "        logits_per_image = output.logits_per_image\n",
    "        logits_per_text = output.logits_per_text\n",
    "        \n",
    "        # image_predictions = torch.argmax(logits_per_image, dim=1)\n",
    "\n",
    "        # Compute loss\n",
    "        total_loss = (loss_img(logits_per_image, labels))\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "# Validation loop -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    print(\"Validation\")\n",
    "    num_correct = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    \n",
    "    pbar = tqdm(test_loader, total=len(test_loader))\n",
    "    for batch in pbar:\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            images,labels = batch \n",
    "            images= images.to(device)\n",
    "            labels= labels.to(device).long()\n",
    "            # Encode image and text\n",
    "            # image_features = model.encode_image(image)\n",
    "            # text_features = model.encode_text(text)\n",
    "            \n",
    "            # Calculate similarity scores between image and text\n",
    "            output = model(text, images)\n",
    "            logits_per_image = output.logits_per_image\n",
    "            logits_per_text = output.logits_per_text\n",
    "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "            predicted_classes = probs.argmax(axis=1)\n",
    "            num_correct = np.sum(predicted_classes == labels.numpy())\n",
    "            \n",
    "            all_probs.extend(predicted_classes)\n",
    "            all_labels.extend(labels)\n",
    "            all_scores.extend(probs[:, 1])\n",
    "\n",
    "    print(classification_report(all_labels, all_probs))\n",
    "    print(f\"AUC: {roc_auc_score(all_labels, all_scores)}\")\n",
    "    print()\n",
    "    \n",
    "    torch.save(model.state_dict(), f'model{epoch}.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for running a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitstudent\\AppData\\Local\\Temp\\ipykernel_19760\\3949199058.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:58<00:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.34      0.50       365\n",
      "           1       0.64      0.98      0.78       447\n",
      "\n",
      "    accuracy                           0.69       812\n",
      "   macro avg       0.78      0.66      0.64       812\n",
      "weighted avg       0.77      0.69      0.65       812\n",
      "\n",
      "AUC: 0.8053905795102817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load in the model\n",
    "model = CLIPModel.from_pretrained(\"vinid/plip\")\n",
    "model.load_state_dict(torch.load(\"model36.pth\"))\n",
    "\n",
    "# test the model\n",
    "print(\"Validation\")\n",
    "num_correct = 0\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "all_scores = []\n",
    "\n",
    "pbar = tqdm(test_loader, total=len(test_loader))\n",
    "for batch in pbar:\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        images,labels = batch \n",
    "        images= images.to(device)\n",
    "        labels= labels.to(device).long()\n",
    "        # Encode image and text\n",
    "        # image_features = model.encode_image(image)\n",
    "        # text_features = model.encode_text(text)\n",
    "        \n",
    "        # Calculate similarity scores between image and text\n",
    "        output = model(text, images)\n",
    "        logits_per_image = output.logits_per_image\n",
    "        logits_per_text = output.logits_per_text\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "        predicted_classes = probs.argmax(axis=1)\n",
    "        num_correct = np.sum(predicted_classes == labels.numpy())\n",
    "        \n",
    "        all_probs.extend(predicted_classes)\n",
    "        all_labels.extend(labels)\n",
    "        all_scores.extend(probs[:, 1])\n",
    "\n",
    "print(classification_report(all_labels, all_probs))\n",
    "print(f\"AUC: {roc_auc_score(all_labels, all_scores)}\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sasp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
