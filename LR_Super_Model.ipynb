{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install imbalanced-learn\n",
    "\n",
    "# commented out pips\n",
    "# replaced the data_path and base_image_path, tan_directory, df1, df2, final_df.to_csv\n",
    "# Please make sure to only add this file, and not the other green ones. I don't want to brake stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, UnidentifiedImageError \n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "import efficientnet_pytorch as Efficinet\n",
    "\n",
    "# Set paths\n",
    "data_path = \"data_NOH.csv\"  # '/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv'\n",
    "base_image_path = \"\" # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(clip.__file__)\n",
    "\n",
    "# Load the CLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to process a single image and text entry through CLIP\n",
    "def process_image_and_text(image_path, text, model, preprocess):\n",
    "    # Open image with error handling for corrupt files\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Error opening image {image_path}: {e}. Skipping.\")\n",
    "        return None, None  # Return None to indicate failure\n",
    "\n",
    "    # Process text\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    return image_features.cpu().numpy(), text_features.cpu().numpy()\n",
    "\n",
    "# Initialize the list to keep track of successfully processed image paths\n",
    "processed_image_paths = []\n",
    "\n",
    "# Iterate over the dataframe and compute embeddings with progress bar\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "# Wrap the iteration in tqdm for progress tracking\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\", unit=\"row\"):\n",
    "    img_path = os.path.join(base_image_path, row['image_path'].replace('\\\\', '/'))\n",
    "    diagnosis_text = row['Surgery diagnosis']\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # Process image and text\n",
    "        img_embed, txt_embed = process_image_and_text(img_path, diagnosis_text, model, preprocess)\n",
    "        \n",
    "        if img_embed is not None and txt_embed is not None:\n",
    "            image_embeddings.append(img_embed)\n",
    "            text_embeddings.append(txt_embed)\n",
    "            # Append the successful image path to the list\n",
    "            processed_image_paths.append(row['image_path'])\n",
    "    else:\n",
    "        print(f\"Image {img_path} does not exist. Skipping.\")\n",
    "\n",
    "# Convert embeddings to arrays and save for later use\n",
    "if image_embeddings:\n",
    "    image_embeddings = torch.cat([torch.tensor(x) for x in image_embeddings], dim=0)\n",
    "    torch.save(image_embeddings, 'image_embeddings.pt')\n",
    "\n",
    "if text_embeddings:\n",
    "    text_embeddings = torch.cat([torch.tensor(x) for x in text_embeddings], dim=0)\n",
    "    torch.save(text_embeddings, 'text_embeddings.pt')\n",
    "\n",
    "print(\"Embeddings saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "# Check the shape of the image embeddings\n",
    "print(image_embeddings.shape)\n",
    "# Check the number of image embeddings generated\n",
    "print(len(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the embeddings\n",
    "image_embeddings = torch.load('image_embeddings.pt')\n",
    "text_embeddings = torch.load('text_embeddings.pt')\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_embeddings, labels, test_size=0.25, random_state=42, stratify=labels)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model with class weight and regularization\n",
    "clf = LogisticRegression(class_weight='balanced', C=0.8)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "print(\"ROC-AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the TAN directory\n",
    "tan_directory = \"TAN\" # '/home/iambrink/NOH_Thyroid_Cancer_Data/TAN'  # Update this path to your TAN folder\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(tan_directory):\n",
    "    if filename.startswith(\"TAN\"):  # Check if the filename starts with \"TAN\"\n",
    "        # Remove the \"TAN\" prefix\n",
    "        new_filename = filename.replace(\"TAN\", \"\", 1)  # Remove \"TAN\" only once\n",
    "        new_filename = new_filename.lstrip(\"\\\\\")  # Remove any leading backslashes if necessary\n",
    "        \n",
    "        # Create full paths for renaming\n",
    "        old_file = os.path.join(tan_directory, filename)\n",
    "        new_file = os.path.join(tan_directory, new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(old_file, new_file)\n",
    "\n",
    "print(\"Files renamed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 =  pd.read_csv('data_NOH.csv') # pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('data_TAN.csv') # pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_TAN_V2.csv')\n",
    "\n",
    "# Select relevant columns from df1\n",
    "columns_df1 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',\n",
    "    'image_path',  # Keep the image_path from df1\n",
    "    'Surgery diagnosis'\n",
    "]\n",
    "\n",
    "# Select relevant columns from df2\n",
    "columns_df2 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',  # Include relevant columns from df2\n",
    "    'image_path',                    # Keep the image_path from df2\n",
    "    'Surgery diagnosis'\n",
    "]\n",
    "\n",
    "# Create DataFrames with only the selected columns\n",
    "df1_selected = df1[columns_df1]\n",
    "df2_selected = df2[columns_df2]\n",
    "\n",
    "# Get the maximum patient number from df1\n",
    "max_patient_number = df1_selected['Patient #'].max()\n",
    "\n",
    "# Update patient numbers in df2 by adding max_patient_number\n",
    "df2_selected['Patient #'] = df2_selected['Patient #'] + max_patient_number\n",
    "\n",
    "# Concatenate the two DataFrames vertically\n",
    "final_df = pd.concat([df1_selected, df2_selected], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "# final_df.to_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "final_df.to_csv('Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "\n",
    "print(\"Super CSV file with adjusted Patient # has been created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 = pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_TAN_V2.csv')\n",
    "\n",
    "# Select relevant columns from df1\n",
    "columns_df1 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',\n",
    "    'image_path'  # Keep the image_path from df1\n",
    "]\n",
    "\n",
    "# Select relevant columns from df2\n",
    "columns_df2 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',  # Include relevant columns from df2\n",
    "    'image_path'                    # Keep the image_path from df2\n",
    "]\n",
    "\n",
    "# Create DataFrames with only the selected columns\n",
    "df1_selected = df1[columns_df1]\n",
    "df2_selected = df2[columns_df2]\n",
    "\n",
    "# Get the maximum patient number from df1\n",
    "max_patient_number = df1_selected['Patient #'].max()\n",
    "\n",
    "# Update patient numbers in df2 by adding max_patient_number\n",
    "df2_selected['Patient #'] = df2_selected['Patient #'] + max_patient_number\n",
    "\n",
    "# Adjust image_path in df2_selected to format it correctly\n",
    "df2_selected['image_path'] = df2_selected['image_path'].str.replace(r'TAN/', '', regex=True)  # Remove 'TAN/' prefix\n",
    "\n",
    "# Extract the numeric part; fill NaN values with empty strings to avoid TypeError\n",
    "df2_selected['image_number'] = df2_selected['image_path'].str.extract(r'(\\d+)')  # Extract the numeric part\n",
    "\n",
    "# Construct the new image_path without duplicating 'TAN'\n",
    "df2_selected['image_path'] = 'TAN\\\\' + df2_selected['image_number'].fillna('Unknown') + '\\\\' + df2_selected['image_path'].str.replace(r'TAN\\d*\\\\', '', regex=True)\n",
    "\n",
    "# Drop the image_number column\n",
    "df2_selected = df2_selected.drop(columns=['image_number'])\n",
    "\n",
    "# Concatenate the two DataFrames vertically\n",
    "final_df = pd.concat([df1_selected, df2_selected], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "final_df.to_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "\n",
    "print(\"Super CSV file with adjusted Patient # and image_path has been created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, UnidentifiedImageError \n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "\n",
    "# Set paths\n",
    "data_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'  # Update to your super CSV file\n",
    "base_image_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/'  # Adjust based on where your images are located\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Load the CLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to process a single image and text entry through CLIP\n",
    "def process_image_and_text(image_path, text, model, preprocess):\n",
    "    # Open image with error handling for corrupt files\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Error opening image {image_path}: {e}. Skipping.\")\n",
    "        return None, None  # Return None to indicate failure\n",
    "\n",
    "    # Process text\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    return image_features.cpu().numpy(), text_features.cpu().numpy()\n",
    "\n",
    "# Initialize the list to keep track of successfully processed image paths\n",
    "processed_image_paths = []\n",
    "\n",
    "# Iterate over the dataframe and compute embeddings with progress bar\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "# Wrap the iteration in tqdm for progress tracking\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\", unit=\"row\"):\n",
    "    # Construct the full image path\n",
    "    img_path = os.path.join(base_image_path, row['image_path'].replace('\\\\', '/'))\n",
    "    diagnosis_text = str(row['Surgery diagnosis in number'])  # Use surgery diagnosis in number as text\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # Process image and text\n",
    "        img_embed, txt_embed = process_image_and_text(img_path, diagnosis_text, model, preprocess)\n",
    "        \n",
    "        if img_embed is not None and txt_embed is not None:\n",
    "            image_embeddings.append(img_embed)\n",
    "            text_embeddings.append(txt_embed)\n",
    "            # Append the successful image path to the list\n",
    "            processed_image_paths.append(row['image_path'])\n",
    "    else:\n",
    "        print(f\"Image {img_path} does not exist. Skipping.\")\n",
    "\n",
    "# Convert embeddings to arrays and save for later use\n",
    "if image_embeddings:\n",
    "    image_embeddings = torch.cat([torch.tensor(x) for x in image_embeddings], dim=0)\n",
    "    torch.save(image_embeddings, 'image_embeddings.pt')\n",
    "\n",
    "if text_embeddings:\n",
    "    text_embeddings = torch.cat([torch.tensor(x) for x in text_embeddings], dim=0)\n",
    "    torch.save(text_embeddings, 'text_embeddings.pt')\n",
    "\n",
    "print(\"Embeddings saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "# Check the shape of the image embeddings\n",
    "print(image_embeddings.shape)\n",
    "# Check the number of image embeddings generated\n",
    "print(len(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the embeddings\n",
    "image_embeddings = torch.load('image_embeddings.pt')\n",
    "text_embeddings = torch.load('text_embeddings.pt')\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have your image_embeddings and labels defined\n",
    "# Filter out rows where labels are NaN\n",
    "valid_indices = ~np.isnan(labels)\n",
    "image_embeddings_filtered = image_embeddings[valid_indices]\n",
    "labels_filtered = labels[valid_indices]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_embeddings_filtered, labels_filtered, test_size=0.25, random_state=42, stratify=labels_filtered)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression(class_weight='balanced', C=0.8)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Save the model\n",
    "dump(clf, 'logistic_regression_model.joblib')  # Using joblib\n",
    "# or\n",
    "# with open('logistic_regression_model.pkl', 'wb') as file:  # Using pickle\n",
    "#     pickle.dump(clf, file)\n",
    "\n",
    "# Load the model\n",
    "clf_loaded = load('logistic_regression_model.joblib')  # Using joblib\n",
    "# or\n",
    "# with open('logistic_regression_model.pkl', 'rb') as file:  # Using pickle\n",
    "#     clf_loaded = pickle.load(file)\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "y_pred = clf_loaded.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_test, clf_loaded.predict_proba(X_test)[:, 1])\n",
    "print(\"ROC-AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # Assuming RGB images\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, 1)  # Binary classification (output one value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 56 * 56)  # Flatten the tensor\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match CNN input size\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pdb\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from plip import PLIP\n",
    "import numpy as np\n",
    "\n",
    "plip = PLIP('vinid/plip')\n",
    "\n",
    "plip_model = CLIPModel.from_pretrained(\"vinid/plip\")\n",
    "plip_processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n",
    "plip_texts = np.unique(df['Surgery diagnosis']).tolist()\n",
    "\n",
    "# Set paths\n",
    "data_path = 'Thyroid_Cancer_TAN&NOH_file.csv' #'/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'\n",
    "base_image_path = '' # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match CNN input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create your own Dataset class to load images and labels\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, base_path, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.base_path, self.dataframe.iloc[idx]['image_path'].replace('\\\\', '/'))\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx]['Surgery diagnosis in number']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load data and prepare for training\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['Surgery diagnosis in number'])  # Drop rows with NaN labels\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = CustomDataset(train_df, base_image_path, transform=transform)\n",
    "test_dataset = CustomDataset(test_df, base_image_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "\n",
    "# Training loop with tqdm\n",
    "num_epochs = 200\n",
    "best_val_acc = 0.0  # To keep track of the best validation accuracy\n",
    "\n",
    "# Heat map using captum\n",
    "\n",
    "heat_map_data_iter = iter(test_loader)\n",
    "heat_map_images, heat_map_labels = next(heat_map_data_iter)\n",
    "test_img = heat_map_images[0]\n",
    "test_label = heat_map_labels[0]\n",
    "\n",
    "output = model(test_img)\n",
    "pred_label_idx = output.argmax(dim=1).item()\n",
    "\n",
    "occlusion = Occlusion(model)\n",
    "\n",
    "# Compute the attributions\n",
    "attributions_occ = occlusion.attribute(\n",
    "    test_img,\n",
    "    target=pred_label_idx,\n",
    "    strides=(16, 16),  # Spatial dimensions only\n",
    "    sliding_window_shapes=(32, 32),  # Spatial dimensions only\n",
    "    baselines=0\n",
    ")\n",
    "\n",
    "# Overlay the attribution on top of the original image with the chosen colormap\n",
    "_ = viz.visualize_image_attr(\n",
    "    np.expand_dims(attributions_occ.squeeze().cpu().detach().numpy(), axis=-1),\n",
    "    np.transpose(test_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "    method=\"blended_heat_map\",\n",
    "    sign=\"all\",\n",
    "    show_colorbar=True,\n",
    "    title=\"Original Image with Colored Attribution Overlay\",\n",
    "    fig_size=(8, 8),\n",
    "    use_pyplot=True,\n",
    "    alpha_overlay=0.5  # Control the transparency of the overlay (0 is fully transparent, 1 is fully opaque)\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.float().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update training metrics\n",
    "            train_loss += loss.item()\n",
    "            predictions = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)  # Increment the progress bar\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = correct_predictions / len(train_dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels.float().to(device))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predictions = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "            val_correct_predictions += (predictions == labels.float().to(device)).sum().item()\n",
    "\n",
    "            val_labels.extend(labels.numpy())\n",
    "            val_preds.extend(predictions.cpu().numpy())\n",
    "            \n",
    "            # add in plip guesses\n",
    "            for image in images:\n",
    "                plip_inputs = plip_processor(text=plip_texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "                plip_outputs = plip_model(**plip_inputs)\n",
    "                plip_logits_per_image = plip_outputs.logits_per_image  # this is the image-text similarity score\n",
    "                plip_probs = plip_logits_per_image.softmax(dim=1) \n",
    "                plip_response = plip_texts[np.argmax(plip_probs.detach().numpy() )]\n",
    "                print(plip_response)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(test_loader)\n",
    "    val_acc = val_correct_predictions / len(test_dataset)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Heat map using captum\n",
    "\n",
    "    heat_map_data_iter = iter(test_loader)\n",
    "    heat_map_images, heat_map_labels = next(heat_map_data_iter)\n",
    "    test_img = heat_map_images[0]\n",
    "    test_label = heat_map_labels[0]\n",
    "\n",
    "    output = model(test_img)\n",
    "    pred_label_idx = output.argmax(dim=1).item()\n",
    "\n",
    "    occlusion = Occlusion(model)\n",
    "\n",
    "    # Compute the attributions\n",
    "    attributions_occ = occlusion.attribute(\n",
    "        test_img,\n",
    "        target=pred_label_idx,\n",
    "        strides=(16, 16),  # Spatial dimensions only\n",
    "        sliding_window_shapes=(32, 32),  # Spatial dimensions only\n",
    "        baselines=0\n",
    "    )\n",
    "\n",
    "    # Overlay the attribution on top of the original image with the chosen colormap\n",
    "    _ = viz.visualize_image_attr(\n",
    "        np.expand_dims(attributions_occ.squeeze().cpu().detach().numpy(), axis=-1),\n",
    "        np.transpose(test_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "        method=\"blended_heat_map\",\n",
    "        sign=\"all\",\n",
    "        show_colorbar=True,\n",
    "        title=\"Original Image with Colored Attribution Overlay\",\n",
    "        fig_size=(8, 8),\n",
    "        use_pyplot=True,\n",
    "        alpha_overlay=0.5  # Control the transparency of the overlay (0 is fully transparent, 1 is fully opaque)\n",
    "    )\n",
    "\n",
    "    # Save the model if the validation accuracy improves\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'super_CNN_Model.pth')\n",
    "        print(\"Model saved with best validation accuracy!\")\n",
    "\n",
    "# Final evaluation\n",
    "model.load_state_dict(torch.load('super_CNN_Model.pth'))  # Load the best model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        predictions = torch.round(torch.sigmoid(outputs.squeeze()))  # Convert probabilities to binary predictions\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_true, y_pred)\n",
    "print(\"ROC-AUC Score:\", auc_score)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "#100 ep - 325 mins, best val acc 88.18% #200 ep - 682 min, best val acc 88.41%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model and evaluate\n",
    "model = SimpleCNN().to(device)\n",
    "model.load_state_dict(torch.load('super_CNN_Model.pth'))  # Load the best model\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Example inference on a single image\n",
    "def predict_image(image_path, model, transform, device):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title('Input Image')\n",
    "    plt.show()\n",
    "\n",
    "    # Transform and add batch dimension\n",
    "    image_transformed = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image_transformed)\n",
    "        probability = torch.sigmoid(output).item()  # Get the probability (confidence between 0-1)\n",
    "        \n",
    "        # Classify based on the probability threshold (0.5 by default)\n",
    "        if probability >= 0.5:\n",
    "            prediction = 'Cancer'\n",
    "            confidence = probability\n",
    "        else:\n",
    "            prediction = 'Non-Cancer'\n",
    "            confidence = 1 - probability  # Confidence for non-cancer is 1 - probability\n",
    "            \n",
    "    return prediction, confidence\n",
    "\n",
    "# Use the prediction function\n",
    "sample_image_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/TAN/001/IMG_20220623_134910.jpg'  # Replace with actual path\n",
    "predicted_class, confidence = predict_image(sample_image_path, model, transform, device)\n",
    "print(f'Predicted Class: {predicted_class}, Confidence: {confidence:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the plip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/77 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x32 and 512x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 139\u001b[0m\n\u001b[0;32m    136\u001b[0m texts \u001b[38;5;241m=\u001b[39m texts\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m logits_per_image, logits_per_text \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# image_predictions = torch.argmax(logits_per_image, dim=1)\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits_per_image\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[25], line 71\u001b[0m, in \u001b[0;36mCLIPBinaryClassifier.forward\u001b[1;34m(self, input_ids, pixel_values)\u001b[0m\n\u001b[0;32m     68\u001b[0m logits_per_image \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits_per_image\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Pass logits through the classifier to reduce the number of classes to 2\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m logits_per_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits_per_image\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, 2]\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Return the logits for both image and text\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits_per_image, outputs\u001b[38;5;241m.\u001b[39mlogits_per_text\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x32 and 512x2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pdb\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from plip import PLIP\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Set paths\n",
    "data_path = 'Thyroid_Cancer_TAN&NOH_file.csv' #'/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'\n",
    "base_image_path = '' # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "class CLIPBinaryClassifier(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super(CLIPBinaryClassifier, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        \n",
    "        hidden_size = clip_model.text_model.config.hidden_size  \n",
    "              \n",
    "        # Modify the final layer to produce 2 logits for binary classification\n",
    "        self.classifier = nn.Linear(hidden_size, 2)  # 2 output classes\n",
    "        \n",
    "    def forward(self, input_ids, pixel_values):\n",
    "        # Get the outputs from the CLIP model\n",
    "        outputs = self.clip_model(input_ids=input_ids, pixel_values=pixel_values)\n",
    "        \n",
    "        # Get the logits per image (for classification)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        \n",
    "        # Pass logits through the classifier to reduce the number of classes to 2\n",
    "        logits_per_image = self.classifier(logits_per_image)  # [batch_size, 2]\n",
    "        \n",
    "        # Return the logits for both image and text\n",
    "        return logits_per_image, outputs.logits_per_text\n",
    "\n",
    "# Load the pretrained CLIP model\n",
    "clip_model = CLIPModel.from_pretrained(\"vinid/plip\")\n",
    "# Wrap the model with the binary classifier\n",
    "model = CLIPBinaryClassifier(clip_model)\n",
    "processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n",
    " \n",
    "class image_title_dataset():\n",
    "    def __init__(self, dataframe, base_path):\n",
    "        self.image_paths = [os.path.join(base_path, row['image_path'].replace('\\\\', '/')) for _, row in dataframe.iterrows()]\n",
    "        self.titles = [clip.tokenize('benign') if row['Surgery diagnosis in number'] == 0 else clip.tokenize('cancer') for _, row in dataframe.iterrows()]\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)), \n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and preprocess\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        image = self.preprocess(image)     \n",
    "        title = self.titles[idx]\n",
    "        \n",
    "        return image, title\n",
    "\n",
    "# Load data and prepare for training\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['Surgery diagnosis in number'])  # Drop rows with NaN labels\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = image_title_dataset(train_df, base_image_path)\n",
    "test_dataset = image_title_dataset(test_df, base_image_path)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images,texts = batch \n",
    "        images= images.to(device)\n",
    "        texts= texts.to(device)\n",
    "        texts = texts.squeeze(1)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(input_ids=texts, pixel_values=images)\n",
    "\n",
    "        # image_predictions = torch.argmax(logits_per_image, dim=1)\n",
    "        print(logits_per_image.shape)\n",
    "\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "# Validation loop -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sasp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
