{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install imbalanced-learn\n",
    "\n",
    "# commented out pips\n",
    "# replaced the data_path and base_image_path, tan_directory, df1, df2, final_df.to_csv\n",
    "# Please make sure to only add this file, and not the other green ones. I don't want to brake stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, UnidentifiedImageError \n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "import efficientnet_pytorch as Efficinet\n",
    "\n",
    "# Set paths\n",
    "data_path = \"data_NOH.csv\"  # '/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv'\n",
    "base_image_path = \"\" # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(clip.__file__)\n",
    "\n",
    "# Load the CLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to process a single image and text entry through CLIP\n",
    "def process_image_and_text(image_path, text, model, preprocess):\n",
    "    # Open image with error handling for corrupt files\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Error opening image {image_path}: {e}. Skipping.\")\n",
    "        return None, None  # Return None to indicate failure\n",
    "\n",
    "    # Process text\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    return image_features.cpu().numpy(), text_features.cpu().numpy()\n",
    "\n",
    "# Initialize the list to keep track of successfully processed image paths\n",
    "processed_image_paths = []\n",
    "\n",
    "# Iterate over the dataframe and compute embeddings with progress bar\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "# Wrap the iteration in tqdm for progress tracking\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\", unit=\"row\"):\n",
    "    img_path = os.path.join(base_image_path, row['image_path'].replace('\\\\', '/'))\n",
    "    diagnosis_text = row['Surgery diagnosis']\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # Process image and text\n",
    "        img_embed, txt_embed = process_image_and_text(img_path, diagnosis_text, model, preprocess)\n",
    "        \n",
    "        if img_embed is not None and txt_embed is not None:\n",
    "            image_embeddings.append(img_embed)\n",
    "            text_embeddings.append(txt_embed)\n",
    "            # Append the successful image path to the list\n",
    "            processed_image_paths.append(row['image_path'])\n",
    "    else:\n",
    "        print(f\"Image {img_path} does not exist. Skipping.\")\n",
    "\n",
    "# Convert embeddings to arrays and save for later use\n",
    "if image_embeddings:\n",
    "    image_embeddings = torch.cat([torch.tensor(x) for x in image_embeddings], dim=0)\n",
    "    torch.save(image_embeddings, 'image_embeddings.pt')\n",
    "\n",
    "if text_embeddings:\n",
    "    text_embeddings = torch.cat([torch.tensor(x) for x in text_embeddings], dim=0)\n",
    "    torch.save(text_embeddings, 'text_embeddings.pt')\n",
    "\n",
    "print(\"Embeddings saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "# Check the shape of the image embeddings\n",
    "print(image_embeddings.shape)\n",
    "# Check the number of image embeddings generated\n",
    "print(len(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the embeddings\n",
    "image_embeddings = torch.load('image_embeddings.pt')\n",
    "text_embeddings = torch.load('text_embeddings.pt')\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_embeddings, labels, test_size=0.25, random_state=42, stratify=labels)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model with class weight and regularization\n",
    "clf = LogisticRegression(class_weight='balanced', C=0.8)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "print(\"ROC-AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the TAN directory\n",
    "tan_directory = \"TAN\" # '/home/iambrink/NOH_Thyroid_Cancer_Data/TAN'  # Update this path to your TAN folder\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(tan_directory):\n",
    "    if filename.startswith(\"TAN\"):  # Check if the filename starts with \"TAN\"\n",
    "        # Remove the \"TAN\" prefix\n",
    "        new_filename = filename.replace(\"TAN\", \"\", 1)  # Remove \"TAN\" only once\n",
    "        new_filename = new_filename.lstrip(\"\\\\\")  # Remove any leading backslashes if necessary\n",
    "        \n",
    "        # Create full paths for renaming\n",
    "        old_file = os.path.join(tan_directory, filename)\n",
    "        new_file = os.path.join(tan_directory, new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(old_file, new_file)\n",
    "\n",
    "print(\"Files renamed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 =  pd.read_csv('data_NOH.csv') # pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('data_TAN.csv') # pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_TAN_V2.csv')\n",
    "\n",
    "# Select relevant columns from df1\n",
    "columns_df1 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',\n",
    "    'image_path',  # Keep the image_path from df1\n",
    "    'Surgery diagnosis'\n",
    "]\n",
    "\n",
    "# Select relevant columns from df2\n",
    "columns_df2 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',  # Include relevant columns from df2\n",
    "    'image_path',                    # Keep the image_path from df2\n",
    "    'Surgery diagnosis'\n",
    "]\n",
    "\n",
    "# Create DataFrames with only the selected columns\n",
    "df1_selected = df1[columns_df1]\n",
    "df2_selected = df2[columns_df2]\n",
    "\n",
    "# Get the maximum patient number from df1\n",
    "max_patient_number = df1_selected['Patient #'].max()\n",
    "\n",
    "# Update patient numbers in df2 by adding max_patient_number\n",
    "df2_selected['Patient #'] = df2_selected['Patient #'] + max_patient_number\n",
    "\n",
    "# Concatenate the two DataFrames vertically\n",
    "final_df = pd.concat([df1_selected, df2_selected], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "# final_df.to_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "final_df.to_csv('Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "\n",
    "print(\"Super CSV file with adjusted Patient # has been created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 = pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_TAN_V2.csv')\n",
    "\n",
    "# Select relevant columns from df1\n",
    "columns_df1 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',\n",
    "    'image_path'  # Keep the image_path from df1\n",
    "]\n",
    "\n",
    "# Select relevant columns from df2\n",
    "columns_df2 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',  # Include relevant columns from df2\n",
    "    'image_path'                    # Keep the image_path from df2\n",
    "]\n",
    "\n",
    "# Create DataFrames with only the selected columns\n",
    "df1_selected = df1[columns_df1]\n",
    "df2_selected = df2[columns_df2]\n",
    "\n",
    "# Get the maximum patient number from df1\n",
    "max_patient_number = df1_selected['Patient #'].max()\n",
    "\n",
    "# Update patient numbers in df2 by adding max_patient_number\n",
    "df2_selected['Patient #'] = df2_selected['Patient #'] + max_patient_number\n",
    "\n",
    "# Adjust image_path in df2_selected to format it correctly\n",
    "df2_selected['image_path'] = df2_selected['image_path'].str.replace(r'TAN/', '', regex=True)  # Remove 'TAN/' prefix\n",
    "\n",
    "# Extract the numeric part; fill NaN values with empty strings to avoid TypeError\n",
    "df2_selected['image_number'] = df2_selected['image_path'].str.extract(r'(\\d+)')  # Extract the numeric part\n",
    "\n",
    "# Construct the new image_path without duplicating 'TAN'\n",
    "df2_selected['image_path'] = 'TAN\\\\' + df2_selected['image_number'].fillna('Unknown') + '\\\\' + df2_selected['image_path'].str.replace(r'TAN\\d*\\\\', '', regex=True)\n",
    "\n",
    "# Drop the image_number column\n",
    "df2_selected = df2_selected.drop(columns=['image_number'])\n",
    "\n",
    "# Concatenate the two DataFrames vertically\n",
    "final_df = pd.concat([df1_selected, df2_selected], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "final_df.to_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "\n",
    "print(\"Super CSV file with adjusted Patient # and image_path has been created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, UnidentifiedImageError \n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "\n",
    "# Set paths\n",
    "data_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'  # Update to your super CSV file\n",
    "base_image_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/'  # Adjust based on where your images are located\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Load the CLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to process a single image and text entry through CLIP\n",
    "def process_image_and_text(image_path, text, model, preprocess):\n",
    "    # Open image with error handling for corrupt files\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Error opening image {image_path}: {e}. Skipping.\")\n",
    "        return None, None  # Return None to indicate failure\n",
    "\n",
    "    # Process text\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    return image_features.cpu().numpy(), text_features.cpu().numpy()\n",
    "\n",
    "# Initialize the list to keep track of successfully processed image paths\n",
    "processed_image_paths = []\n",
    "\n",
    "# Iterate over the dataframe and compute embeddings with progress bar\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "# Wrap the iteration in tqdm for progress tracking\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\", unit=\"row\"):\n",
    "    # Construct the full image path\n",
    "    img_path = os.path.join(base_image_path, row['image_path'].replace('\\\\', '/'))\n",
    "    diagnosis_text = str(row['Surgery diagnosis in number'])  # Use surgery diagnosis in number as text\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # Process image and text\n",
    "        img_embed, txt_embed = process_image_and_text(img_path, diagnosis_text, model, preprocess)\n",
    "        \n",
    "        if img_embed is not None and txt_embed is not None:\n",
    "            image_embeddings.append(img_embed)\n",
    "            text_embeddings.append(txt_embed)\n",
    "            # Append the successful image path to the list\n",
    "            processed_image_paths.append(row['image_path'])\n",
    "    else:\n",
    "        print(f\"Image {img_path} does not exist. Skipping.\")\n",
    "\n",
    "# Convert embeddings to arrays and save for later use\n",
    "if image_embeddings:\n",
    "    image_embeddings = torch.cat([torch.tensor(x) for x in image_embeddings], dim=0)\n",
    "    torch.save(image_embeddings, 'image_embeddings.pt')\n",
    "\n",
    "if text_embeddings:\n",
    "    text_embeddings = torch.cat([torch.tensor(x) for x in text_embeddings], dim=0)\n",
    "    torch.save(text_embeddings, 'text_embeddings.pt')\n",
    "\n",
    "print(\"Embeddings saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "# Check the shape of the image embeddings\n",
    "print(image_embeddings.shape)\n",
    "# Check the number of image embeddings generated\n",
    "print(len(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the embeddings\n",
    "image_embeddings = torch.load('image_embeddings.pt')\n",
    "text_embeddings = torch.load('text_embeddings.pt')\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have your image_embeddings and labels defined\n",
    "# Filter out rows where labels are NaN\n",
    "valid_indices = ~np.isnan(labels)\n",
    "image_embeddings_filtered = image_embeddings[valid_indices]\n",
    "labels_filtered = labels[valid_indices]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_embeddings_filtered, labels_filtered, test_size=0.25, random_state=42, stratify=labels_filtered)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression(class_weight='balanced', C=0.8)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Save the model\n",
    "dump(clf, 'logistic_regression_model.joblib')  # Using joblib\n",
    "# or\n",
    "# with open('logistic_regression_model.pkl', 'wb') as file:  # Using pickle\n",
    "#     pickle.dump(clf, file)\n",
    "\n",
    "# Load the model\n",
    "clf_loaded = load('logistic_regression_model.joblib')  # Using joblib\n",
    "# or\n",
    "# with open('logistic_regression_model.pkl', 'rb') as file:  # Using pickle\n",
    "#     clf_loaded = pickle.load(file)\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "y_pred = clf_loaded.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_test, clf_loaded.predict_proba(X_test)[:, 1])\n",
    "print(\"ROC-AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # Assuming RGB images\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, 1)  # Binary classification (output one value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 56 * 56)  # Flatten the tensor\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match CNN input size\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:  12%|█▏        | 9/77 [01:20<10:10,  8.98s/batch, loss=0.611]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 156\u001b[0m\n\u001b[0;32m    153\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m--> 156\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\gjwhi\\miniconda3\\envs\\sasp_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\gjwhi\\miniconda3\\envs\\sasp_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\gjwhi\\miniconda3\\envs\\sasp_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\gjwhi\\miniconda3\\envs\\sasp_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 59\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     58\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 59\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSurgery diagnosis in number\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[1;32mc:\\Users\\gjwhi\\miniconda3\\envs\\sasp_env\\Lib\\site-packages\\PIL\\Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gjwhi\\miniconda3\\envs\\sasp_env\\Lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pdb\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from plip import PLIP\n",
    "import numpy as np\n",
    "\n",
    "# Set paths\n",
    "data_path = 'Thyroid_Cancer_TAN&NOH_file.csv' #'/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'\n",
    "base_image_path = '' # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match CNN input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create your own Dataset class to load images and labels\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, base_path, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.base_path, self.dataframe.iloc[idx]['image_path'].replace('\\\\', '/'))\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx]['Surgery diagnosis in number']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def saveHeatMap(img, label, model, save_path, filename):\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    # Initialize Occlusion\n",
    "    occlusion = Occlusion(model)\n",
    "    \n",
    "    # Get model prediction\n",
    "    output = model(img)\n",
    "    pred_label_idx = output.argmax(dim=1).item()\n",
    "\n",
    "    # Compute the attributions\n",
    "    attributions_occ = occlusion.attribute(\n",
    "        img,\n",
    "        target=pred_label_idx,\n",
    "        strides=(16, 16),  # Spatial dimensions only\n",
    "        sliding_window_shapes=(32, 32),  # Spatial dimensions only\n",
    "        baselines=0\n",
    "    )\n",
    "\n",
    "    # Save the original image with title\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.title(f'Patient with {\"cancer\" if label == 1 else \"no cancer\"}')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(save_path, f'{filename}original.png'), bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Save the heatmap overlay\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    _ = viz.visualize_image_attr(\n",
    "        np.expand_dims(attributions_occ.squeeze().cpu().detach().numpy(), axis=-1),\n",
    "        np.transpose(img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "        method=\"blended_heat_map\",\n",
    "        sign=\"all\",\n",
    "        show_colorbar=True,\n",
    "        title=f'Patient with {\"cancer\" if label == 1 else \"no cancer\"}',\n",
    "        use_pyplot=False,  # Disable interactive display\n",
    "        alpha_overlay=0.5,  # Transparency of the overlay\n",
    "        plt_fig_axis=(fig, ax)\n",
    "    )\n",
    "    fig.savefig(os.path.join(save_path, f'{filename}heatmap.png'), bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Load data and prepare for training\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['Surgery diagnosis in number'])  # Drop rows with NaN labels\n",
    "\n",
    "plip = PLIP('vinid/plip')\n",
    "\n",
    "plip_model = CLIPModel.from_pretrained(\"vinid/plip\")\n",
    "plip_processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n",
    "plip_texts = np.unique(df['Surgery diagnosis']).tolist()\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = CustomDataset(train_df, base_image_path, transform=transform)\n",
    "test_dataset = CustomDataset(test_df, base_image_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "\n",
    "# Training loop with tqdm\n",
    "num_epochs = 200\n",
    "best_val_acc = 0.0  # To keep track of the best validation accuracy\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.float().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update training metrics\n",
    "            train_loss += loss.item()\n",
    "            predictions = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)  # Increment the progress bar\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = correct_predictions / len(train_dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels.float().to(device))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predictions = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "            val_correct_predictions += (predictions == labels.float().to(device)).sum().item()\n",
    "\n",
    "            val_labels.extend(labels.numpy())\n",
    "            val_preds.extend(predictions.cpu().numpy())\n",
    "            \n",
    "            # add in plip guesses\n",
    "            # for image in images:\n",
    "            #     plip_inputs = plip_processor(text=plip_texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "            #     plip_outputs = plip_model(**plip_inputs)\n",
    "            #     plip_logits_per_image = plip_outputs.logits_per_image  # this is the image-text similarity score\n",
    "            #     plip_probs = plip_logits_per_image.softmax(dim=1) \n",
    "            #     plip_response = plip_texts[np.argmax(plip_probs.detach().numpy() )]\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(test_loader)\n",
    "    val_acc = val_correct_predictions / len(test_dataset)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Heat map using captum\n",
    "    if epoch == 2:\n",
    "        heat_map_data_iter = iter(test_loader)\n",
    "        heat_map_images, heat_map_labels = next(heat_map_data_iter)\n",
    "        for i in range(len(heat_map_images)):\n",
    "            saveHeatMap( heat_map_images[i], heat_map_labels[i], model, \"heat_maps\", i)\n",
    "\n",
    "    # Save the model if the validation accuracy improves\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'super_CNN_Model.pth')\n",
    "        print(\"Model saved with best validation accuracy!\")\n",
    "\n",
    "# Final evaluation\n",
    "model.load_state_dict(torch.load('super_CNN_Model.pth'))  # Load the best model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        predictions = torch.round(torch.sigmoid(outputs.squeeze()))  # Convert probabilities to binary predictions\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_true, y_pred)\n",
    "print(\"ROC-AUC Score:\", auc_score)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "#100 ep - 325 mins, best val acc 88.18% #200 ep - 682 min, best val acc 88.41%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model and evaluate\n",
    "model = SimpleCNN().to(device)\n",
    "model.load_state_dict(torch.load('super_CNN_Model.pth'))  # Load the best model\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Example inference on a single image\n",
    "def predict_image(image_path, model, transform, device):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title('Input Image')\n",
    "    plt.show()\n",
    "\n",
    "    # Transform and add batch dimension\n",
    "    image_transformed = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image_transformed)\n",
    "        probability = torch.sigmoid(output).item()  # Get the probability (confidence between 0-1)\n",
    "        \n",
    "        # Classify based on the probability threshold (0.5 by default)\n",
    "        if probability >= 0.5:\n",
    "            prediction = 'Cancer'\n",
    "            confidence = probability\n",
    "        else:\n",
    "            prediction = 'Non-Cancer'\n",
    "            confidence = 1 - probability  # Confidence for non-cancer is 1 - probability\n",
    "            \n",
    "    return prediction, confidence\n",
    "\n",
    "# Use the prediction function\n",
    "sample_image_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/TAN/001/IMG_20220623_134910.jpg'  # Replace with actual path\n",
    "predicted_class, confidence = predict_image(sample_image_path, model, transform, device)\n",
    "print(f'Predicted Class: {predicted_class}, Confidence: {confidence:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the plip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pdb\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from plip import PLIP\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Set paths\n",
    "data_path = 'Thyroid_Cancer_TAN&NOH_file.csv' #'/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'\n",
    "base_image_path = '' # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# class CLIPBinaryClassifier(nn.Module):\n",
    "#     def __init__(self, clip_model):\n",
    "#         super(CLIPBinaryClassifier, self).__init__()\n",
    "#         self.clip_model = clip_model\n",
    "        \n",
    "#         hidden_size = clip_model.text_model.config.hidden_size  \n",
    "              \n",
    "#         # Modify the final layer to produce 2 logits for binary classification\n",
    "#         self.classifier = nn.Linear(hidden_size, 2)  # 2 output classes\n",
    "        \n",
    "#     def forward(self, input_ids, pixel_values):\n",
    "#         # Get the outputs from the CLIP model\n",
    "#         outputs = self.clip_model(input_ids=input_ids, pixel_values=pixel_values)\n",
    "        \n",
    "#         # Get the logits per image (for classification)\n",
    "#         logits_per_image = outputs.logits_per_image\n",
    "        \n",
    "#         # Pass logits through the classifier to reduce the number of classes to 2\n",
    "#         logits_per_image = self.classifier(logits_per_image)  # [batch_size, 2]\n",
    "        \n",
    "#         # Return the logits for both image and text\n",
    "#         return logits_per_image, outputs.logits_per_text\n",
    "\n",
    "# Load the pretrained CLIP model\n",
    "model = CLIPModel.from_pretrained(\"vinid/plip\")\n",
    "# Wrap the model with the binary classifier\n",
    "# model = CLIPBinaryClassifier(clip_model)\n",
    "processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n",
    " \n",
    "class image_title_dataset():\n",
    "    def __init__(self, dataframe, base_path):\n",
    "        self.image_paths = [os.path.join(base_path, row['image_path'].replace('\\\\', '/')) for _, row in dataframe.iterrows()]\n",
    "        # self.labels = [clip.tokenize('benign') if row['Surgery diagnosis in number'] == 0 else clip.tokenize('cancer') for _, row in dataframe.iterrows()]\n",
    "        self.labels = [ row['Surgery diagnosis in number']  for _, row in dataframe.iterrows()]\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)), \n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and preprocess\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        image = self.preprocess(image)     \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Load data and prepare for training\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['Surgery diagnosis in number'])  # Drop rows with NaN labels\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = image_title_dataset(train_df, base_image_path)\n",
    "test_dataset = image_title_dataset(test_df, base_image_path)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "# the text classes the model will be using\n",
    "text = clip.tokenize([\"benign\", \"cancer\"]).to(device)\n",
    "\n",
    "# Training loop -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print(\"Training\")\n",
    "    pbar = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images,labels = batch \n",
    "        images= images.to(device)\n",
    "        labels= labels.to(device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(text, pixel_values=images)\n",
    "\n",
    "        logits_per_image = output.logits_per_image\n",
    "        logits_per_text = output.logits_per_text\n",
    "        \n",
    "        # image_predictions = torch.argmax(logits_per_image, dim=1)\n",
    "\n",
    "        # Compute loss\n",
    "        total_loss = (loss_img(logits_per_image, labels))\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "# Validation loop -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    print(\"Validation\")\n",
    "    num_correct = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    \n",
    "    pbar = tqdm(test_loader, total=len(test_loader))\n",
    "    for batch in pbar:\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            images,labels = batch \n",
    "            images= images.to(device)\n",
    "            labels= labels.to(device).long()\n",
    "            # Encode image and text\n",
    "            # image_features = model.encode_image(image)\n",
    "            # text_features = model.encode_text(text)\n",
    "            \n",
    "            # Calculate similarity scores between image and text\n",
    "            output = model(text, images)\n",
    "            logits_per_image = output.logits_per_image\n",
    "            logits_per_text = output.logits_per_text\n",
    "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "            predicted_classes = probs.argmax(axis=1)\n",
    "            num_correct = np.sum(predicted_classes == labels.numpy())\n",
    "            \n",
    "            all_probs.extend(predicted_classes)\n",
    "            all_labels.extend(labels)\n",
    "            all_scores.extend(probs[:, 1])\n",
    "\n",
    "    print(classification_report(all_labels, all_probs))\n",
    "    print(f\"AUC: {roc_auc_score(all_labels, all_scores)}\")\n",
    "    print()\n",
    "    \n",
    "    torch.save(model.state_dict(), f'model{epoch}.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for running a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load in the model\n",
    "model = CLIPModel.from_pretrained(\"vinid/plip\")\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "# test the model\n",
    "print(\"Validation\")\n",
    "num_correct = 0\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "all_scores = []\n",
    "\n",
    "pbar = tqdm(test_loader, total=len(test_loader))\n",
    "for batch in pbar:\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        images,labels = batch \n",
    "        images= images.to(device)\n",
    "        labels= labels.to(device).long()\n",
    "        # Encode image and text\n",
    "        # image_features = model.encode_image(image)\n",
    "        # text_features = model.encode_text(text)\n",
    "        \n",
    "        # Calculate similarity scores between image and text\n",
    "        output = model(text, images)\n",
    "        logits_per_image = output.logits_per_image\n",
    "        logits_per_text = output.logits_per_text\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "        predicted_classes = probs.argmax(axis=1)\n",
    "        num_correct = np.sum(predicted_classes == labels.numpy())\n",
    "        \n",
    "        all_probs.extend(predicted_classes)\n",
    "        all_labels.extend(labels)\n",
    "        all_scores.extend(probs[:, 1])\n",
    "\n",
    "print(classification_report(all_labels, all_probs))\n",
    "print(f\"AUC: {roc_auc_score(all_labels, all_scores)}\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sasp_env)",
   "language": "python",
   "name": "sasp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
