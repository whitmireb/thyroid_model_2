{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install imbalanced-learn\n",
    "\n",
    "# commented out pips\n",
    "# replaced the data_path and base_image_path, tan_directory, df1, df2, final_df.to_csv\n",
    "# Please make sure to only add this file, and not the other green ones. I don't want to brake stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, UnidentifiedImageError \n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "import efficientnet_pytorch as Efficinet\n",
    "\n",
    "# Set paths\n",
    "data_path = \"data_NOH.csv\"  # '/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv'\n",
    "base_image_path = \"\" # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(clip.__file__)\n",
    "\n",
    "# Load the CLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to process a single image and text entry through CLIP\n",
    "def process_image_and_text(image_path, text, model, preprocess):\n",
    "    # Open image with error handling for corrupt files\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Error opening image {image_path}: {e}. Skipping.\")\n",
    "        return None, None  # Return None to indicate failure\n",
    "\n",
    "    # Process text\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    return image_features.cpu().numpy(), text_features.cpu().numpy()\n",
    "\n",
    "# Initialize the list to keep track of successfully processed image paths\n",
    "processed_image_paths = []\n",
    "\n",
    "# Iterate over the dataframe and compute embeddings with progress bar\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "# Wrap the iteration in tqdm for progress tracking\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\", unit=\"row\"):\n",
    "    img_path = os.path.join(base_image_path, row['image_path'].replace('\\\\', '/'))\n",
    "    diagnosis_text = row['Surgery diagnosis']\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # Process image and text\n",
    "        img_embed, txt_embed = process_image_and_text(img_path, diagnosis_text, model, preprocess)\n",
    "        \n",
    "        if img_embed is not None and txt_embed is not None:\n",
    "            image_embeddings.append(img_embed)\n",
    "            text_embeddings.append(txt_embed)\n",
    "            # Append the successful image path to the list\n",
    "            processed_image_paths.append(row['image_path'])\n",
    "    else:\n",
    "        print(f\"Image {img_path} does not exist. Skipping.\")\n",
    "\n",
    "# Convert embeddings to arrays and save for later use\n",
    "if image_embeddings:\n",
    "    image_embeddings = torch.cat([torch.tensor(x) for x in image_embeddings], dim=0)\n",
    "    torch.save(image_embeddings, 'image_embeddings.pt')\n",
    "\n",
    "if text_embeddings:\n",
    "    text_embeddings = torch.cat([torch.tensor(x) for x in text_embeddings], dim=0)\n",
    "    torch.save(text_embeddings, 'text_embeddings.pt')\n",
    "\n",
    "print(\"Embeddings saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "# Check the shape of the image embeddings\n",
    "print(image_embeddings.shape)\n",
    "# Check the number of image embeddings generated\n",
    "print(len(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the embeddings\n",
    "image_embeddings = torch.load('image_embeddings.pt')\n",
    "text_embeddings = torch.load('text_embeddings.pt')\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_embeddings, labels, test_size=0.25, random_state=42, stratify=labels)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model with class weight and regularization\n",
    "clf = LogisticRegression(class_weight='balanced', C=0.8)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "print(\"ROC-AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the TAN directory\n",
    "tan_directory = \"TAN\" # '/home/iambrink/NOH_Thyroid_Cancer_Data/TAN'  # Update this path to your TAN folder\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(tan_directory):\n",
    "    if filename.startswith(\"TAN\"):  # Check if the filename starts with \"TAN\"\n",
    "        # Remove the \"TAN\" prefix\n",
    "        new_filename = filename.replace(\"TAN\", \"\", 1)  # Remove \"TAN\" only once\n",
    "        new_filename = new_filename.lstrip(\"\\\\\")  # Remove any leading backslashes if necessary\n",
    "        \n",
    "        # Create full paths for renaming\n",
    "        old_file = os.path.join(tan_directory, filename)\n",
    "        new_file = os.path.join(tan_directory, new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(old_file, new_file)\n",
    "\n",
    "print(\"Files renamed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 =  pd.read_csv('data_NOH.csv') # pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('data_TAN.csv') # pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_TAN_V2.csv')\n",
    "\n",
    "# Select relevant columns from df1\n",
    "columns_df1 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',\n",
    "    'image_path',  # Keep the image_path from df1\n",
    "    'Surgery diagnosis'\n",
    "]\n",
    "\n",
    "# Select relevant columns from df2\n",
    "columns_df2 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',  # Include relevant columns from df2\n",
    "    'image_path',                    # Keep the image_path from df2\n",
    "    'Surgery diagnosis'\n",
    "]\n",
    "\n",
    "# Create DataFrames with only the selected columns\n",
    "df1_selected = df1[columns_df1]\n",
    "df2_selected = df2[columns_df2]\n",
    "\n",
    "# Get the maximum patient number from df1\n",
    "max_patient_number = df1_selected['Patient #'].max()\n",
    "\n",
    "# Update patient numbers in df2 by adding max_patient_number\n",
    "df2_selected['Patient #'] = df2_selected['Patient #'] + max_patient_number\n",
    "\n",
    "# Concatenate the two DataFrames vertically\n",
    "final_df = pd.concat([df1_selected, df2_selected], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "# final_df.to_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "final_df.to_csv('Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "\n",
    "print(\"Super CSV file with adjusted Patient # has been created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 = pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_NOH_V2.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/data_TAN_V2.csv')\n",
    "\n",
    "# Select relevant columns from df1\n",
    "columns_df1 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',\n",
    "    'image_path'  # Keep the image_path from df1\n",
    "]\n",
    "\n",
    "# Select relevant columns from df2\n",
    "columns_df2 = [\n",
    "    'Patient #',\n",
    "    'Surgery diagnosis in number',  # Include relevant columns from df2\n",
    "    'image_path'                    # Keep the image_path from df2\n",
    "]\n",
    "\n",
    "# Create DataFrames with only the selected columns\n",
    "df1_selected = df1[columns_df1]\n",
    "df2_selected = df2[columns_df2]\n",
    "\n",
    "# Get the maximum patient number from df1\n",
    "max_patient_number = df1_selected['Patient #'].max()\n",
    "\n",
    "# Update patient numbers in df2 by adding max_patient_number\n",
    "df2_selected['Patient #'] = df2_selected['Patient #'] + max_patient_number\n",
    "\n",
    "# Adjust image_path in df2_selected to format it correctly\n",
    "df2_selected['image_path'] = df2_selected['image_path'].str.replace(r'TAN/', '', regex=True)  # Remove 'TAN/' prefix\n",
    "\n",
    "# Extract the numeric part; fill NaN values with empty strings to avoid TypeError\n",
    "df2_selected['image_number'] = df2_selected['image_path'].str.extract(r'(\\d+)')  # Extract the numeric part\n",
    "\n",
    "# Construct the new image_path without duplicating 'TAN'\n",
    "df2_selected['image_path'] = 'TAN\\\\' + df2_selected['image_number'].fillna('Unknown') + '\\\\' + df2_selected['image_path'].str.replace(r'TAN\\d*\\\\', '', regex=True)\n",
    "\n",
    "# Drop the image_number column\n",
    "df2_selected = df2_selected.drop(columns=['image_number'])\n",
    "\n",
    "# Concatenate the two DataFrames vertically\n",
    "final_df = pd.concat([df1_selected, df2_selected], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "final_df.to_csv('/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv', index=False)\n",
    "\n",
    "print(\"Super CSV file with adjusted Patient # and image_path has been created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, UnidentifiedImageError \n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "\n",
    "# Set paths\n",
    "data_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'  # Update to your super CSV file\n",
    "base_image_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/'  # Adjust based on where your images are located\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Load the CLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to process a single image and text entry through CLIP\n",
    "def process_image_and_text(image_path, text, model, preprocess):\n",
    "    # Open image with error handling for corrupt files\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Error opening image {image_path}: {e}. Skipping.\")\n",
    "        return None, None  # Return None to indicate failure\n",
    "\n",
    "    # Process text\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    return image_features.cpu().numpy(), text_features.cpu().numpy()\n",
    "\n",
    "# Initialize the list to keep track of successfully processed image paths\n",
    "processed_image_paths = []\n",
    "\n",
    "# Iterate over the dataframe and compute embeddings with progress bar\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "# Wrap the iteration in tqdm for progress tracking\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\", unit=\"row\"):\n",
    "    # Construct the full image path\n",
    "    img_path = os.path.join(base_image_path, row['image_path'].replace('\\\\', '/'))\n",
    "    diagnosis_text = str(row['Surgery diagnosis in number'])  # Use surgery diagnosis in number as text\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # Process image and text\n",
    "        img_embed, txt_embed = process_image_and_text(img_path, diagnosis_text, model, preprocess)\n",
    "        \n",
    "        if img_embed is not None and txt_embed is not None:\n",
    "            image_embeddings.append(img_embed)\n",
    "            text_embeddings.append(txt_embed)\n",
    "            # Append the successful image path to the list\n",
    "            processed_image_paths.append(row['image_path'])\n",
    "    else:\n",
    "        print(f\"Image {img_path} does not exist. Skipping.\")\n",
    "\n",
    "# Convert embeddings to arrays and save for later use\n",
    "if image_embeddings:\n",
    "    image_embeddings = torch.cat([torch.tensor(x) for x in image_embeddings], dim=0)\n",
    "    torch.save(image_embeddings, 'image_embeddings.pt')\n",
    "\n",
    "if text_embeddings:\n",
    "    text_embeddings = torch.cat([torch.tensor(x) for x in text_embeddings], dim=0)\n",
    "    torch.save(text_embeddings, 'text_embeddings.pt')\n",
    "\n",
    "print(\"Embeddings saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "# Check the shape of the image embeddings\n",
    "print(image_embeddings.shape)\n",
    "# Check the number of image embeddings generated\n",
    "print(len(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the embeddings\n",
    "image_embeddings = torch.load('image_embeddings.pt')\n",
    "text_embeddings = torch.load('text_embeddings.pt')\n",
    "valid_indices = df.index[df['image_path'].isin(processed_image_paths)].tolist()\n",
    "labels = df['Surgery diagnosis in number'].values[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have your image_embeddings and labels defined\n",
    "# Filter out rows where labels are NaN\n",
    "valid_indices = ~np.isnan(labels)\n",
    "image_embeddings_filtered = image_embeddings[valid_indices]\n",
    "labels_filtered = labels[valid_indices]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_embeddings_filtered, labels_filtered, test_size=0.25, random_state=42, stratify=labels_filtered)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression(class_weight='balanced', C=0.8)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Save the model\n",
    "dump(clf, 'logistic_regression_model.joblib')  # Using joblib\n",
    "# or\n",
    "# with open('logistic_regression_model.pkl', 'wb') as file:  # Using pickle\n",
    "#     pickle.dump(clf, file)\n",
    "\n",
    "# Load the model\n",
    "clf_loaded = load('logistic_regression_model.joblib')  # Using joblib\n",
    "# or\n",
    "# with open('logistic_regression_model.pkl', 'rb') as file:  # Using pickle\n",
    "#     clf_loaded = pickle.load(file)\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "y_pred = clf_loaded.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_test, clf_loaded.predict_proba(X_test)[:, 1])\n",
    "print(\"ROC-AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # Assuming RGB images\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, 1)  # Binary classification (output one value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 56 * 56)  # Flatten the tensor\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match CNN input size\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pdb\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from plip import PLIP\n",
    "import numpy as np\n",
    "\n",
    "plip = PLIP('vinid/plip')\n",
    "\n",
    "plip_model = CLIPModel.from_pretrained(\"vinid/plip\")\n",
    "plip_processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n",
    "plip_texts = np.unique(df['Surgery diagnosis']).tolist()\n",
    "\n",
    "# Set paths\n",
    "data_path = 'Thyroid_Cancer_TAN&NOH_file.csv' #'/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'\n",
    "base_image_path = '' # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match CNN input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create your own Dataset class to load images and labels\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, base_path, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.base_path, self.dataframe.iloc[idx]['image_path'].replace('\\\\', '/'))\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx]['Surgery diagnosis in number']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load data and prepare for training\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['Surgery diagnosis in number'])  # Drop rows with NaN labels\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = CustomDataset(train_df, base_image_path, transform=transform)\n",
    "test_dataset = CustomDataset(test_df, base_image_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "\n",
    "# Training loop with tqdm\n",
    "num_epochs = 200\n",
    "best_val_acc = 0.0  # To keep track of the best validation accuracy\n",
    "\n",
    "# Heat map using captum\n",
    "\n",
    "heat_map_data_iter = iter(test_loader)\n",
    "heat_map_images, heat_map_labels = next(heat_map_data_iter)\n",
    "test_img = heat_map_images[0]\n",
    "test_label = heat_map_labels[0]\n",
    "\n",
    "output = model(test_img)\n",
    "pred_label_idx = output.argmax(dim=1).item()\n",
    "\n",
    "occlusion = Occlusion(model)\n",
    "\n",
    "# Compute the attributions\n",
    "attributions_occ = occlusion.attribute(\n",
    "    test_img,\n",
    "    target=pred_label_idx,\n",
    "    strides=(16, 16),  # Spatial dimensions only\n",
    "    sliding_window_shapes=(32, 32),  # Spatial dimensions only\n",
    "    baselines=0\n",
    ")\n",
    "\n",
    "# Overlay the attribution on top of the original image with the chosen colormap\n",
    "_ = viz.visualize_image_attr(\n",
    "    np.expand_dims(attributions_occ.squeeze().cpu().detach().numpy(), axis=-1),\n",
    "    np.transpose(test_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "    method=\"blended_heat_map\",\n",
    "    sign=\"all\",\n",
    "    show_colorbar=True,\n",
    "    title=\"Original Image with Colored Attribution Overlay\",\n",
    "    fig_size=(8, 8),\n",
    "    use_pyplot=True,\n",
    "    alpha_overlay=0.5  # Control the transparency of the overlay (0 is fully transparent, 1 is fully opaque)\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.float().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update training metrics\n",
    "            train_loss += loss.item()\n",
    "            predictions = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)  # Increment the progress bar\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = correct_predictions / len(train_dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels.float().to(device))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predictions = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "            val_correct_predictions += (predictions == labels.float().to(device)).sum().item()\n",
    "\n",
    "            val_labels.extend(labels.numpy())\n",
    "            val_preds.extend(predictions.cpu().numpy())\n",
    "            \n",
    "            # add in plip guesses\n",
    "            for image in images:\n",
    "                plip_inputs = plip_processor(text=plip_texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "                plip_outputs = plip_model(**plip_inputs)\n",
    "                plip_logits_per_image = plip_outputs.logits_per_image  # this is the image-text similarity score\n",
    "                plip_probs = plip_logits_per_image.softmax(dim=1) \n",
    "                plip_response = plip_texts[np.argmax(plip_probs.detach().numpy() )]\n",
    "                print(plip_response)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(test_loader)\n",
    "    val_acc = val_correct_predictions / len(test_dataset)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Heat map using captum\n",
    "\n",
    "    heat_map_data_iter = iter(test_loader)\n",
    "    heat_map_images, heat_map_labels = next(heat_map_data_iter)\n",
    "    test_img = heat_map_images[0]\n",
    "    test_label = heat_map_labels[0]\n",
    "\n",
    "    output = model(test_img)\n",
    "    pred_label_idx = output.argmax(dim=1).item()\n",
    "\n",
    "    occlusion = Occlusion(model)\n",
    "\n",
    "    # Compute the attributions\n",
    "    attributions_occ = occlusion.attribute(\n",
    "        test_img,\n",
    "        target=pred_label_idx,\n",
    "        strides=(16, 16),  # Spatial dimensions only\n",
    "        sliding_window_shapes=(32, 32),  # Spatial dimensions only\n",
    "        baselines=0\n",
    "    )\n",
    "\n",
    "    # Overlay the attribution on top of the original image with the chosen colormap\n",
    "    _ = viz.visualize_image_attr(\n",
    "        np.expand_dims(attributions_occ.squeeze().cpu().detach().numpy(), axis=-1),\n",
    "        np.transpose(test_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "        method=\"blended_heat_map\",\n",
    "        sign=\"all\",\n",
    "        show_colorbar=True,\n",
    "        title=\"Original Image with Colored Attribution Overlay\",\n",
    "        fig_size=(8, 8),\n",
    "        use_pyplot=True,\n",
    "        alpha_overlay=0.5  # Control the transparency of the overlay (0 is fully transparent, 1 is fully opaque)\n",
    "    )\n",
    "\n",
    "    # Save the model if the validation accuracy improves\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'super_CNN_Model.pth')\n",
    "        print(\"Model saved with best validation accuracy!\")\n",
    "\n",
    "# Final evaluation\n",
    "model.load_state_dict(torch.load('super_CNN_Model.pth'))  # Load the best model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        predictions = torch.round(torch.sigmoid(outputs.squeeze()))  # Convert probabilities to binary predictions\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Evaluate with ROC-AUC score\n",
    "auc_score = roc_auc_score(y_true, y_pred)\n",
    "print(\"ROC-AUC Score:\", auc_score)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "#100 ep - 325 mins, best val acc 88.18% #200 ep - 682 min, best val acc 88.41%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model and evaluate\n",
    "model = SimpleCNN().to(device)\n",
    "model.load_state_dict(torch.load('super_CNN_Model.pth'))  # Load the best model\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Example inference on a single image\n",
    "def predict_image(image_path, model, transform, device):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title('Input Image')\n",
    "    plt.show()\n",
    "\n",
    "    # Transform and add batch dimension\n",
    "    image_transformed = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image_transformed)\n",
    "        probability = torch.sigmoid(output).item()  # Get the probability (confidence between 0-1)\n",
    "        \n",
    "        # Classify based on the probability threshold (0.5 by default)\n",
    "        if probability >= 0.5:\n",
    "            prediction = 'Cancer'\n",
    "            confidence = probability\n",
    "        else:\n",
    "            prediction = 'Non-Cancer'\n",
    "            confidence = 1 - probability  # Confidence for non-cancer is 1 - probability\n",
    "            \n",
    "    return prediction, confidence\n",
    "\n",
    "# Use the prediction function\n",
    "sample_image_path = '/home/iambrink/NOH_Thyroid_Cancer_Data/TAN/001/IMG_20220623_134910.jpg'  # Replace with actual path\n",
    "predicted_class, confidence = predict_image(sample_image_path, model, transform, device)\n",
    "print(f'Predicted Class: {predicted_class}, Confidence: {confidence:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the plip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/77 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts shape:  torch.Size([32, 1, 77])\n",
      "texts shape:  torch.Size([32, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 138\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts shape: \u001b[39m\u001b[38;5;124m'\u001b[39m, texts\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m logits_per_image, logits_per_text \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m    141\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(images),dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong,device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:1363\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[1;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m   1358\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1359\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1360\u001b[0m )\n\u001b[0;32m   1361\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1363\u001b[0m vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1371\u001b[0m text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_model(\n\u001b[0;32m   1372\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1373\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1377\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1378\u001b[0m )\n\u001b[0;32m   1380\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:1094\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[1;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1094\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m   1097\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1098\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m   1099\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1100\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1101\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1102\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\pitstudent\\Desktop\\Hoan 1\\sasp_env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:242\u001b[0m, in \u001b[0;36mCLIPVisionEmbeddings.forward\u001b[1;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values: torch\u001b[38;5;241m.\u001b[39mFloatTensor, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 242\u001b[0m     batch_size, _, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interpolate_pos_encoding \u001b[38;5;129;01mand\u001b[39;00m (height \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;129;01mor\u001b[39;00m width \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size):\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    245\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pdb\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from plip import PLIP\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"vinid/plip\")\n",
    "processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n",
    "\n",
    "# Set paths\n",
    "data_path = 'Thyroid_Cancer_TAN&NOH_file.csv' #'/home/iambrink/NOH_Thyroid_Cancer_Data/Thyroid_Cancer_TAN&NOH_file.csv'\n",
    "base_image_path = '' # '/home/iambrink/NOH_Thyroid_Cancer_Data/'\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match CNN input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "    \n",
    "# class image_title_dataset():\n",
    "#     def __init__(self, dataframe, base_path):\n",
    "#         # Initialize image paths and corresponding texts\n",
    "#         self.image_path = os.path.join(base_path, dataframe.iloc[idx]['image_path'].replace('\\\\', '/'))\n",
    "#         # Tokenize text using CLIP's tokenizer\n",
    "#         self.title  = clip.tokenize('benign' if dataframe.iloc[idx]['Surgery diagnosis in number'] == 0 else 'cancer')\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.title)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Preprocess image using CLIP's preprocessing function\n",
    "#         image = preprocess(Image.open(self.image_path[idx]))\n",
    "#         title = self.title[idx]\n",
    "#         return image, title\n",
    "\n",
    "class image_title_dataset():\n",
    "    def __init__(self, dataframe, base_path):\n",
    "        self.image_paths = [os.path.join(base_path, row['image_path'].replace('\\\\', '/')) for _, row in dataframe.iterrows()]\n",
    "        self.titles = [clip.tokenize('benign' if row['Surgery diagnosis in number'] == 0 else 'cancer') for _, row in dataframe.iterrows()]\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)), \n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and preprocess\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        image = self.preprocess(image)     \n",
    "        title = self.titles[idx]\n",
    "        \n",
    "        return image, title\n",
    "\n",
    "# Load data and prepare for training\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['Surgery diagnosis in number'])  # Drop rows with NaN labels\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = image_title_dataset(train_df, base_image_path)\n",
    "test_dataset = image_title_dataset(test_df, base_image_path)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# plip = PLIP('vinid/plip')\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images,texts = batch \n",
    "        images= images.to(device)\n",
    "        texts= texts.to(device)\n",
    "        \n",
    "        print('texts shape: ', texts.shape)\n",
    "        texts = texts.squeeze(1)\n",
    "        print('texts shape: ', texts.shape)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sasp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
